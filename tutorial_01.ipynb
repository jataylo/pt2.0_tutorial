{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc735b09-82bf-4259-a7d5-b3ce1407b137",
   "metadata": {},
   "source": [
    "## Python - Frame Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de38bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/rocm5.4.2\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/rocm5.4.2/torch-2.0.1%2Brocm5.4.2-cp38-cp38-linux_x86_64.whl (1536.4 MB)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (0.16.0.dev20230817+rocm5.6)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (2.1.0.dev20230817+rocm5.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.8/lib/python3.8/site-packages/filelock-3.12.2-py3.8.egg (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Collecting pytorch-triton-rocm<2.1,>=2.0.0 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/pytorch_triton_rocm-2.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78.3 MB)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from torchvision) (2.31.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/rocm5.4.2/torchvision-0.15.2%2Brocm5.4.2-cp38-cp38-linux_x86_64.whl (89.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from torchvision) (10.0.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/rocm5.4.2/torchaudio-2.0.2%2Brocm5.4.2-cp38-cp38-linux_x86_64.whl (4.1 MB)\n",
      "Requirement already satisfied: cmake in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py_3.8/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: pytorch-triton-rocm, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0.dev20230817+rocm5.6\n",
      "    Uninstalling torchvision-0.16.0.dev20230817+rocm5.6:\n",
      "      Successfully uninstalled torchvision-0.16.0.dev20230817+rocm5.6\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.1.0.dev20230817+rocm5.6\n",
      "    Uninstalling torchaudio-2.1.0.dev20230817+rocm5.6:\n",
      "      Successfully uninstalled torchaudio-2.1.0.dev20230817+rocm5.6\n",
      "Successfully installed pytorch-triton-rocm-2.0.1 torch-2.0.1+rocm5.4.2 torchaudio-2.0.2+rocm5.4.2 torchvision-0.15.2+rocm5.4.2\n"
     ]
    }
   ],
   "source": [
    "# Install PT2.0 - stable\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4c575-6962-4d3a-b5e7-a4385f52d92c",
   "metadata": {},
   "source": [
    "### 01 - CPython interpreter & Python bytecode\n",
    "Python built-in module `dis` (diassembler) can be used to output the bytecode for python functions\n",
    "\n",
    "The bytecode will show:\n",
    "- The line number of the source code that corresponds to to the byte code\n",
    "- List of instructions the CPython interpreter will execute\n",
    "- Instruction pointers (0,2,4,6) indicate the position of each bytecode instruction in the bytecode sequence (to cover jumps)\n",
    "- Index of local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4177f608-3559-4771-98da-c7a23bd03a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3           0 LOAD_FAST                0 (x)\n",
      "              2 LOAD_FAST                1 (y)\n",
      "              4 BINARY_ADD\n",
      "              6 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "#Foo-style function\n",
    "def foo(x, y):\n",
    "    return x + y\n",
    "\n",
    "import dis\n",
    "dis.dis(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3599b-786f-4e10-be4f-6f98642f3293",
   "metadata": {},
   "source": [
    "We can also compile the function down to Python bytecode\n",
    "- def compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7baccb-5622-4ae3-bf70-24d9fbef46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2           0 LOAD_CONST               5 ((5, 7))\n",
      "              2 LOAD_CONST               2 (<code object foo_compile at 0x7f3438529f50, file \"<string>\", line 2>)\n",
      "              4 LOAD_CONST               3 ('foo_compile')\n",
      "              6 MAKE_FUNCTION            1 (defaults)\n",
      "              8 STORE_NAME               0 (foo_compile)\n",
      "             10 LOAD_CONST               4 (None)\n",
      "             12 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object foo_compile at 0x7f3438529f50, file \"<string>\", line 2>:\n",
      "  3           0 LOAD_FAST                0 (x)\n",
      "              2 LOAD_FAST                1 (y)\n",
      "              4 BINARY_ADD\n",
      "              6 RETURN_VALUE\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "source = \"\"\"\n",
    "def foo_compile(x=5, y=7):\n",
    "    return x + y\n",
    "\"\"\"\n",
    "\n",
    "# filename: '<string>' to denote code string but could be file made\n",
    "# mode: 'exec', 'eval' 'single'\n",
    "code_object = compile(source, '<string>', 'exec')\n",
    "\n",
    "# Run the bytecode in cpython - this example it defines the function\n",
    "exec(code_object)\n",
    "dis.dis(code_object)\n",
    "\n",
    "result = foo_compile(3, 4)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee9bcbc-4977-4be1-8c58-52a1a1f9440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4           0 LOAD_FAST                0 (x)\n",
      "              2 LOAD_FAST                1 (y)\n",
      "              4 BINARY_ADD\n",
      "              6 STORE_FAST               2 (result)\n",
      "\n",
      "  5           8 LOAD_GLOBAL              0 (inspect)\n",
      "             10 LOAD_METHOD              1 (currentframe)\n",
      "             12 CALL_METHOD              0\n",
      "             14 STORE_FAST               3 (frame)\n",
      "\n",
      "  6          16 LOAD_FAST                2 (result)\n",
      "             18 LOAD_FAST                3 (frame)\n",
      "             20 BUILD_TUPLE              2\n",
      "             22 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "def foo_compile(x=5, y=7):\n",
    "    result = x + y\n",
    "    frame = inspect.currentframe()\n",
    "    return result, frame\n",
    "\n",
    "result, pyt_frame = foo_compile(3, 4)\n",
    "\n",
    "dis.dis(foo_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cdd6059-728e-4298-aff2-e4f00bada954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'frame'>\n",
      "Code object: <code object foo_compile at 0x7f3438529c90, file \"/tmp/ipykernel_232813/58056854.py\", line 3>\n",
      "Local variables: {'x': 3, 'y': 4, 'result': 7, 'frame': <frame at 0x7f3438533550, file '/tmp/ipykernel_232813/58056854.py', line 6, code foo_compile>}\n",
      "Global variables: {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', \"# Install PT2.0\\nget_ipython().system('pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2')\", '#Foo-style function\\ndef foo(x, y):\\n    return x + y\\n\\nimport dis\\ndis.dis(foo)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\nbytecode = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nprint(bytecode)\\nexec(bytecode)\\ndis.dis(bytecode)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\nbytecode = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nprint(bytecode)\\nexec(bytecode)\\n#dis.dis(bytecode)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\ncode_object = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nexec(code_object)\\ndis.dis(code_object)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\ncode_object = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nexec(code_object)\\ndis.dis(code_object)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'import inspect\\n\\ndef foo_compile(x=5, y=7):\\n    result = x + y\\n    frame = inspect.currentframe()\\n    return result, frame\\n\\nresult, pyt_frame = foo_compile(3, 4)\\n\\ndis.dis(foo_compile)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\nprint(\"All attributes of frame\")\\nprint(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\nprint(\"All attributes of frame\")\\nprint(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\nprint(\"Global variables:\", frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\nprint(\"Global variables:\", pyt_frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)'], '_oh': {}, '_dh': [PosixPath('/var/lib/jenkins/pt2.0_tutorial'), PosixPath('/var/lib/jenkins/pt2.0_tutorial')], 'In': ['', \"# Install PT2.0\\nget_ipython().system('pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2')\", '#Foo-style function\\ndef foo(x, y):\\n    return x + y\\n\\nimport dis\\ndis.dis(foo)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\nbytecode = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nprint(bytecode)\\nexec(bytecode)\\ndis.dis(bytecode)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\nbytecode = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nprint(bytecode)\\nexec(bytecode)\\n#dis.dis(bytecode)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\ncode_object = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nexec(code_object)\\ndis.dis(code_object)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\ncode_object = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nexec(code_object)\\ndis.dis(code_object)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'import inspect\\n\\ndef foo_compile(x=5, y=7):\\n    result = x + y\\n    frame = inspect.currentframe()\\n    return result, frame\\n\\nresult, pyt_frame = foo_compile(3, 4)\\n\\ndis.dis(foo_compile)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\nprint(\"All attributes of frame\")\\nprint(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\nprint(\"All attributes of frame\")\\nprint(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\nprint(\"Global variables:\", frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)', '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\nprint(\"Global variables:\", pyt_frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f3439e5e280>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f34385a6160>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f34385a6160>, 'open': <function open at 0x7f343aff7280>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/var/lib/jenkins/pt2.0_tutorial/01-Dynamo_AOT.ipynb', '_i': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\nprint(\"Global variables:\", frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)', '_ii': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\nprint(\"All attributes of frame\")\\nprint(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '_iii': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '_i1': '# Install PT2.0\\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2', '_exit_code': 0, '_i2': '#Foo-style function\\ndef foo(x, y):\\n    return x + y\\n\\nimport dis\\ndis.dis(foo)', 'foo': <function foo at 0x7f343856b940>, 'dis': <module 'dis' from '/opt/conda/envs/py_3.8/lib/python3.8/dis.py'>, '_i3': 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\nbytecode = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nprint(bytecode)\\nexec(bytecode)\\ndis.dis(bytecode)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'source': '\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n', 'bytecode': <code object <module> at 0x7f3438529d40, file \"<string>\", line 2>, 'foo_compile': <function foo_compile at 0x7f343856b700>, 'result': 7, '_i4': 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\nbytecode = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nprint(bytecode)\\nexec(bytecode)\\n#dis.dis(bytecode)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', '_i5': 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\ncode_object = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nexec(code_object)\\ndis.dis(code_object)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', 'code_object': <code object <module> at 0x7f343853a030, file \"<string>\", line 2>, '_i6': 'source = \"\"\"\\ndef foo_compile(x=5, y=7):\\n    return x + y\\n\"\"\"\\n\\n# filename: \\'<string>\\' to denote code string but could be file made\\n# mode: \\'exec\\', \\'eval\\' \\'single\\'\\ncode_object = compile(source, \\'<string>\\', \\'exec\\')\\n\\n# Run the bytecode in cpython - this example it defines the function\\nexec(code_object)\\ndis.dis(code_object)\\n\\nresult = foo_compile(3, 4)\\nprint(result)', '_i7': 'import inspect\\n\\ndef foo_compile(x=5, y=7):\\n    result = x + y\\n    frame = inspect.currentframe()\\n    return result, frame\\n\\nresult, pyt_frame = foo_compile(3, 4)\\n\\ndis.dis(foo_compile)', 'inspect': <module 'inspect' from '/opt/conda/envs/py_3.8/lib/python3.8/inspect.py'>, 'pyt_frame': <frame at 0x7f3438533550, file '/tmp/ipykernel_232813/58056854.py', line 6, code foo_compile>, '_i8': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\nprint(\"All attributes of frame\")\\nprint(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)', '_i9': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '_i10': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\nprint(\"All attributes of frame\")\\nprint(pyt_frame.__dir__())\\n\\n#print(\"Code object:\", pyt_frame.f_code)\\n#print(\"Local variables:\", pyt_frame.f_locals)\\n#print(\"Global variables:\", frame.f_globals)\\n#print(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\n#print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\n#print(\"Current line number in Python code:\", pyt_frame.f_lineno)', '_i11': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\nprint(\"Global variables:\", frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)', '_i12': '# PyFrameObject\\nprint(type(pyt_frame))\\n\\n#print(\"All attributes of frame\")\\n#print(pyt_frame.__dir__())\\n\\nprint(\"Code object:\", pyt_frame.f_code)\\nprint(\"Local variables:\", pyt_frame.f_locals)\\nprint(\"Global variables:\", pyt_frame.f_globals)\\nprint(\"Value of x:\", pyt_frame.f_locals[\\'x\\'])\\nprint(\"Back pointer to the previous frame:\", pyt_frame.f_back)\\nprint(\"Current line number in Python code:\", pyt_frame.f_lineno)'}\n",
      "Value of x: 3\n",
      "Back pointer to the previous frame: <frame at 0x7f3438536230, file '/tmp/ipykernel_232813/58056854.py', line 8, code <module>>\n",
      "Current line number in Python code: 6\n"
     ]
    }
   ],
   "source": [
    "# PyFrameObject\n",
    "print(type(pyt_frame))\n",
    "\n",
    "#print(\"All attributes of frame\")\n",
    "#print(pyt_frame.__dir__())\n",
    "\n",
    "print(\"Code object:\", pyt_frame.f_code)\n",
    "print(\"Local variables:\", pyt_frame.f_locals)\n",
    "print(\"Global variables:\", pyt_frame.f_globals)\n",
    "print(\"Value of x:\", pyt_frame.f_locals['x'])\n",
    "print(\"Back pointer to the previous frame:\", pyt_frame.f_back)\n",
    "print(\"Current line number in Python code:\", pyt_frame.f_lineno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a47070-c53c-4a09-8836-0e17c061822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'code'>\n",
      "['__repr__', '__hash__', '__getattribute__', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__new__', '__sizeof__', 'replace', 'co_argcount', 'co_posonlyargcount', 'co_kwonlyargcount', 'co_nlocals', 'co_stacksize', 'co_flags', 'co_code', 'co_consts', 'co_names', 'co_varnames', 'co_freevars', 'co_cellvars', 'co_filename', 'co_name', 'co_firstlineno', 'co_lnotab', '__doc__', '__str__', '__setattr__', '__delattr__', '__init__', '__reduce_ex__', '__reduce__', '__subclasshook__', '__init_subclass__', '__format__', '__dir__', '__class__']\n",
      "========================\n",
      "Actual bytecode as a bytes object\n",
      "b'|\\x00|\\x01\\x17\\x00}\\x02t\\x00\\xa0\\x01\\xa1\\x00}\\x03|\\x02|\\x03f\\x02S\\x00'\n",
      "Constants used in the bytecode\n",
      "(None,)\n",
      "Names of the variables and functions\n",
      "('inspect', 'currentframe')\n",
      "Names of the local variables\n",
      "('x', 'y', 'result', 'frame')\n",
      "Number of args\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#PyCodeObject\n",
    "code_obj = pyt_frame.f_code\n",
    "\n",
    "print(type(code_obj))\n",
    "print(code_obj.__dir__())\n",
    "\n",
    "print(\"========================\")\n",
    "print(\"Actual bytecode as a bytes object\")\n",
    "print(code_obj.co_code)\n",
    "print(\"Constants used in the bytecode\")\n",
    "print(code_obj.co_consts)\n",
    "print(\"Names of the variables and functions\")\n",
    "print(code_obj.co_names)\n",
    "print(\"Names of the local variables\")\n",
    "print(code_obj.co_varnames)\n",
    "print(\"Number of args\")\n",
    "print(code_obj.co_argcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba007d-6dfe-466c-b7ca-6a7477343b88",
   "metadata": {},
   "source": [
    "### 02 - Frame Evaluation API https://peps.python.org/pep-0523/\n",
    "Instead of sending bytecode to interpreter it can be passed to any callback which could modify the bytecode and pass to the interpreter - which is what TorchDynamo does :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab2998-cc3d-436e-b004-798d7f95644d",
   "metadata": {},
   "source": [
    "### 03 - TorchDynamo\n",
    "https://pytorch.org/get-started/pytorch-2.0/\n",
    "\n",
    "Python:\n",
    "- Foo is a function call\n",
    "- This function call becomes a python frame object - each entry of a stack trace essentially\n",
    "- In each PyFrameObject lives a PyCodeObject which says which the files the functions are from, the constants/local variables\n",
    "- _PyEval_EvalFrameDefault() - Takes a frame and evalutes it with the interpreter\n",
    "- (31:44)\n",
    "\n",
    "Dynamo:\n",
    "- Uses the same foo function call, the same PyFrameObject and PyCodeObject\n",
    "- It extracts the torch components of the bytecode and produces several outputs\n",
    "       - It produces the FX Graph\n",
    "       - It produces a compiled function from a user-defined compiler using the lowered FX graph as input\n",
    "       - And it transforms the PyCodeObject bytecode and patches back into the Frame\n",
    "       - Along the way we accumulate guards to avoid excessive recompilation\n",
    "  \n",
    "  \n",
    "![Image](https://pytorch.org/docs/stable/_images/TorchDynamo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f69f28",
   "metadata": {},
   "source": [
    "### 03a - TorchDynamo - FX Graph\n",
    "TorchDynamo intercepts the execution of Python codes (PyFrameObject), transforms them into an FX intermediate representation (IR) and stores them in a special structure called an FX Graph\n",
    "\n",
    "In reality FX graphs are essentially containers for some IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a17166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x : torch.Tensor, y : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_232813/2934430678.py:15, code: a = x * y\n",
      "        mul = x * y;  x = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_232813/2934430678.py:16, code: b = a * y\n",
      "        mul_1 = mul * y;  y = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_232813/2934430678.py:17, code: return a + b\n",
      "        add = mul + mul_1;  mul = mul_1 = None\n",
      "        return (add,)\n",
      "        \n",
      "opcode         name    target                   args          kwargs\n",
      "-------------  ------  -----------------------  ------------  --------\n",
      "placeholder    x       x                        ()            {}\n",
      "placeholder    y       y                        ()            {}\n",
      "call_function  mul     <built-in function mul>  (x, y)        {}\n",
      "call_function  mul_1   <built-in function mul>  (mul, y)      {}\n",
      "call_function  add     <built-in function add>  (mul, mul_1)  {}\n",
      "output         output  output                   ((add,),)     {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:279: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0706, -0.0674, -0.4773, -0.1619, -2.3563])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "import logging\n",
    "from torch import _dynamo\n",
    "\n",
    "torch._dynamo.config.log_level = False #logging.INFO\n",
    "torch._dynamo.config.output_code = False\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs= List[torch.Tensor]):\n",
    "    code = gm.print_readable()\n",
    "    gm.graph.print_tabular()\n",
    "    return gm.forward\n",
    "\n",
    "def toy_example(x, y):\n",
    "    a = x * y\n",
    "    b = a * y\n",
    "    return a + b\n",
    "\n",
    "compiled_fn = torch.compile(toy_example, backend=my_compiler)\n",
    "\n",
    "compiled_fn(torch.randn(5), torch.randn(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67a62c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:279: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect\n",
      "  warnings.warn(\n",
      "[2023-08-18 12:43:10,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing toy_example\n",
      "[2023-08-18 12:43:10,240] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_232813/2571593824.py:15\n",
      "[2023-08-18 12:43:10,241] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []\n",
      "[2023-08-18 12:43:10,242] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST y [TensorVariable()]\n",
      "[2023-08-18 12:43:10,243] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_MULTIPLY None [TensorVariable(), TensorVariable()]\n",
      "[2023-08-18 12:43:10,246] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST a [TensorVariable()]\n",
      "[2023-08-18 12:43:10,247] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_232813/2571593824.py:16\n",
      "[2023-08-18 12:43:10,247] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL print []\n",
      "[2023-08-18 12:43:10,248] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST break! [BuiltinVariable(print)]\n",
      "[2023-08-18 12:43:10,248] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [BuiltinVariable(print), ConstantVariable(str)]\n",
      "[2023-08-18 12:43:10,249] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 342, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 965, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 474, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 585, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/variables/base.py\", line 230, in call_function\n",
      "    unimplemented(f\"call_function {self} {args} {kwargs}\")\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: call_function BuiltinVariable(print) [ConstantVariable(str)] {}\n",
      "[2023-08-18 12:43:10,252] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes\n",
      "[2023-08-18 12:43:10,253] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='call_function BuiltinVariable(print) [ConstantVariable(str)] {}', user_stack=[<FrameSummary file /tmp/ipykernel_232813/2571593824.py, line 16 in toy_example>])\n",
      "[2023-08-18 12:43:10,255] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
      "[2023-08-18 12:43:10,258] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
      "[2023-08-18 12:43:10,259] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
      " __compiled_fn_27 <eval_with_key>.43 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    x       x                        ()         {}\n",
      "placeholder    y       y                        ()         {}\n",
      "call_function  mul     <built-in function mul>  (x, y)     {}\n",
      "output         output  output                   ((mul,),)  {}\n",
      "\n",
      "[2023-08-18 12:43:10,262] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE toy_example /tmp/ipykernel_232813/2571593824.py line 14 \n",
      " 15           0 LOAD_FAST                0 (x)\n",
      "              2 LOAD_FAST                1 (y)\n",
      "              4 BINARY_MULTIPLY\n",
      "              6 STORE_FAST               2 (a)\n",
      "\n",
      " 16           8 LOAD_GLOBAL              0 (print)\n",
      "             10 LOAD_CONST               1 ('break!')\n",
      "             12 CALL_FUNCTION            1\n",
      "             14 POP_TOP\n",
      "\n",
      " 17          16 LOAD_FAST                2 (a)\n",
      "             18 LOAD_FAST                1 (y)\n",
      "             20 BINARY_MULTIPLY\n",
      "             22 STORE_FAST               3 (b)\n",
      "\n",
      " 18          24 LOAD_GLOBAL              1 (torch)\n",
      "             26 LOAD_ATTR                2 (_dynamo)\n",
      "             28 LOAD_METHOD              3 (graph_break)\n",
      "             30 CALL_METHOD              0\n",
      "             32 POP_TOP\n",
      "\n",
      " 19          34 LOAD_FAST                2 (a)\n",
      "             36 LOAD_FAST                3 (b)\n",
      "             38 BINARY_ADD\n",
      "             40 RETURN_VALUE\n",
      "\n",
      " \n",
      "[2023-08-18 12:43:10,263] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE toy_example /tmp/ipykernel_232813/2571593824.py line 14 \n",
      " 14           0 LOAD_GLOBAL              4 (__compiled_fn_27)\n",
      "              2 LOAD_FAST                0 (x)\n",
      "              4 LOAD_FAST                1 (y)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 STORE_FAST               4 (___graph_out_0)\n",
      "             10 LOAD_GLOBAL              0 (print)\n",
      "             12 LOAD_CONST               1 ('break!')\n",
      "             14 LOAD_FAST                4 (___graph_out_0)\n",
      "             16 LOAD_CONST               2 (0)\n",
      "             18 BINARY_SUBSCR\n",
      "             20 STORE_FAST               2 (a)\n",
      "\n",
      " 16          22 CALL_FUNCTION            1\n",
      "             24 LOAD_GLOBAL              5 (__resume_at_14_28)\n",
      "             26 ROT_TWO\n",
      "             28 LOAD_FAST                1 (y)\n",
      "             30 LOAD_FAST                2 (a)\n",
      "             32 CALL_FUNCTION            3\n",
      "             34 RETURN_VALUE\n",
      "\n",
      " \n",
      "[2023-08-18 12:43:10,264] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
      " - \n",
      "            local 'x' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': ['TENSOR_MATCH'],\n",
      "                'code': None,\n",
      "                'obj_weakref': <weakref at 0x7f310fa088b0; to 'Tensor' at 0x7f310fb0b900>\n",
      "                'guarded_class': <weakref at 0x7f343b299b80; to 'torch._C._TensorMeta' at 0x62bccb0 (Tensor)>\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'y' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': ['TENSOR_MATCH'],\n",
      "                'code': None,\n",
      "                'obj_weakref': <weakref at 0x7f310fa08900; to 'Tensor' at 0x7f310fa9a310>\n",
      "                'guarded_class': <weakref at 0x7f343b299b80; to 'torch._C._TensorMeta' at 0x62bccb0 (Tensor)>\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'print' BUILTIN_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-08-18 12:43:10,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in toy_example>\n",
      "[2023-08-18 12:43:10,273] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []\n",
      "[2023-08-18 12:43:10,276] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 18 [ConstantVariable(NoneType)]\n",
      "[2023-08-18 12:43:10,277] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [ConstantVariable(NoneType)]\n",
      "[2023-08-18 12:43:10,278] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_232813/2571593824.py:17\n",
      "[2023-08-18 12:43:10,279] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-08-18 12:43:10,280] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST y [TensorVariable()]\n",
      "[2023-08-18 12:43:10,281] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_MULTIPLY None [TensorVariable(), TensorVariable()]\n",
      "[2023-08-18 12:43:10,285] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST b [TensorVariable()]\n",
      "[2023-08-18 12:43:10,285] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_232813/2571593824.py:18\n",
      "[2023-08-18 12:43:10,286] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-08-18 12:43:10,287] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR _dynamo [TorchVariable(<module 'torch' from '/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/__init__.py'>)]\n",
      "[2023-08-18 12:43:10,288] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR graph_break [PythonModuleVariable()]\n",
      "[2023-08-18 12:43:10,289] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [SkipFilesVariable()]\n",
      "[2023-08-18 12:43:10,290] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 342, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 965, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 474, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/variables/misc.py\", line 795, in call_function\n",
      "    unimplemented(\n",
      "  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: call_function graph_break in skip_files /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/__init__.py\n",
      "[2023-08-18 12:43:10,293] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes\n",
      "[2023-08-18 12:43:10,293] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='call_function graph_break in skip_files /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/__init__.py', user_stack=[<FrameSummary file /tmp/ipykernel_232813/2571593824.py, line 18 in <graph break in toy_example>>])\n",
      "[2023-08-18 12:43:10,297] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
      "[2023-08-18 12:43:10,300] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
      "[2023-08-18 12:43:10,302] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
      " __compiled_fn_29 <eval_with_key>.45 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    y       y                        ()         {}\n",
      "placeholder    a       a                        ()         {}\n",
      "call_function  mul     <built-in function mul>  (a, y)     {}\n",
      "output         output  output                   ((mul,),)  {}\n",
      "\n",
      "[2023-08-18 12:43:10,307] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in toy_example> /tmp/ipykernel_232813/2571593824.py line 16 \n",
      " 16           0 LOAD_FAST                0 (___stack0)\n",
      "              2 JUMP_ABSOLUTE           18\n",
      "              4 LOAD_FAST                3 (x)\n",
      "              6 LOAD_FAST                1 (y)\n",
      "              8 BINARY_MULTIPLY\n",
      "             10 STORE_FAST               2 (a)\n",
      "             12 LOAD_GLOBAL              0 (print)\n",
      "             14 LOAD_CONST               1 ('break!')\n",
      "             16 CALL_FUNCTION            1\n",
      "        >>   18 POP_TOP\n",
      "\n",
      " 17          20 LOAD_FAST                2 (a)\n",
      "             22 LOAD_FAST                1 (y)\n",
      "             24 BINARY_MULTIPLY\n",
      "             26 STORE_FAST               4 (b)\n",
      "\n",
      " 18          28 LOAD_GLOBAL              1 (torch)\n",
      "             30 LOAD_ATTR                2 (_dynamo)\n",
      "             32 LOAD_ATTR                3 (graph_break)\n",
      "             34 CALL_FUNCTION            0\n",
      "             36 POP_TOP\n",
      "\n",
      " 19          38 LOAD_FAST                2 (a)\n",
      "             40 LOAD_FAST                4 (b)\n",
      "             42 BINARY_ADD\n",
      "             44 RETURN_VALUE\n",
      "\n",
      " \n",
      "[2023-08-18 12:43:10,308] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in toy_example> /tmp/ipykernel_232813/2571593824.py line 16 \n",
      " 16           0 LOAD_GLOBAL              4 (__compiled_fn_29)\n",
      "              2 LOAD_FAST                1 (y)\n",
      "              4 LOAD_FAST                2 (a)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 STORE_FAST               5 (___graph_out_0)\n",
      "             10 LOAD_GLOBAL              1 (torch)\n",
      "             12 LOAD_ATTR                2 (_dynamo)\n",
      "             14 LOAD_ATTR                3 (graph_break)\n",
      "             16 LOAD_FAST                5 (___graph_out_0)\n",
      "             18 LOAD_CONST               2 (0)\n",
      "             20 BINARY_SUBSCR\n",
      "             22 STORE_FAST               4 (b)\n",
      "\n",
      " 18          24 CALL_FUNCTION            0\n",
      "             26 LOAD_GLOBAL              5 (__resume_at_36_30)\n",
      "             28 ROT_TWO\n",
      "             30 LOAD_FAST                2 (a)\n",
      "             32 LOAD_FAST                4 (b)\n",
      "             34 CALL_FUNCTION            3\n",
      "             36 RETURN_VALUE\n",
      "\n",
      " \n",
      "[2023-08-18 12:43:10,310] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': ['TENSOR_MATCH'],\n",
      "                'code': None,\n",
      "                'obj_weakref': <weakref at 0x7f300f1d8540; to 'Tensor' at 0x7f310fa084a0>\n",
      "                'guarded_class': <weakref at 0x7f343b299b80; to 'torch._C._TensorMeta' at 0x62bccb0 (Tensor)>\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'y' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': ['TENSOR_MATCH'],\n",
      "                'code': None,\n",
      "                'obj_weakref': <weakref at 0x7f310fa08900; to 'Tensor' at 0x7f310fa9a310>\n",
      "                'guarded_class': <weakref at 0x7f343b299b80; to 'torch._C._TensorMeta' at 0x62bccb0 (Tensor)>\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'torch' FUNCTION_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'torch._dynamo' PYMODULE_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'torch._dynamo.graph_break' FUNCTION_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-08-18 12:43:10,317] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in toy_example>\n",
      "[2023-08-18 12:43:10,318] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []\n",
      "[2023-08-18 12:43:10,318] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 36 [ConstantVariable(NoneType)]\n",
      "[2023-08-18 12:43:10,319] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [ConstantVariable(NoneType)]\n",
      "[2023-08-18 12:43:10,320] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_232813/2571593824.py:19\n",
      "[2023-08-18 12:43:10,320] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-08-18 12:43:10,321] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-08-18 12:43:10,322] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-08-18 12:43:10,327] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-08-18 12:43:10,328] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in toy_example> (RETURN_VALUE)\n",
      "[2023-08-18 12:43:10,328] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-08-18 12:43:10,329] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_232813/2571593824.py, line 19 in <graph break in toy_example>>])\n",
      "[2023-08-18 12:43:10,332] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
      "[2023-08-18 12:43:10,334] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
      "[2023-08-18 12:43:10,336] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
      " __compiled_fn_31 <eval_with_key>.47 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  add     <built-in function add>  (a, b)     {}\n",
      "output         output  output                   ((add,),)  {}\n",
      "\n",
      "[2023-08-18 12:43:10,338] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in toy_example> /tmp/ipykernel_232813/2571593824.py line 18 \n",
      " 18           0 LOAD_FAST                0 (___stack0)\n",
      "              2 JUMP_ABSOLUTE           36\n",
      "              4 LOAD_FAST                3 (x)\n",
      "              6 LOAD_FAST                4 (y)\n",
      "              8 BINARY_MULTIPLY\n",
      "             10 STORE_FAST               1 (a)\n",
      "             12 LOAD_GLOBAL              0 (print)\n",
      "             14 LOAD_CONST               1 ('break!')\n",
      "             16 CALL_FUNCTION            1\n",
      "             18 POP_TOP\n",
      "             20 LOAD_FAST                1 (a)\n",
      "             22 LOAD_FAST                4 (y)\n",
      "             24 BINARY_MULTIPLY\n",
      "             26 STORE_FAST               2 (b)\n",
      "             28 LOAD_GLOBAL              1 (torch)\n",
      "             30 LOAD_ATTR                2 (_dynamo)\n",
      "             32 LOAD_ATTR                3 (graph_break)\n",
      "             34 CALL_FUNCTION            0\n",
      "        >>   36 POP_TOP\n",
      "\n",
      " 19          38 LOAD_FAST                1 (a)\n",
      "             40 LOAD_FAST                2 (b)\n",
      "             42 BINARY_ADD\n",
      "             44 RETURN_VALUE\n",
      "\n",
      " \n",
      "[2023-08-18 12:43:10,339] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in toy_example> /tmp/ipykernel_232813/2571593824.py line 18 \n",
      " 18           0 LOAD_GLOBAL              4 (__compiled_fn_31)\n",
      "              2 LOAD_FAST                1 (a)\n",
      "              4 LOAD_FAST                2 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      " \n",
      "[2023-08-18 12:43:10,340] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': ['TENSOR_MATCH'],\n",
      "                'code': None,\n",
      "                'obj_weakref': <weakref at 0x7f300f1d8540; to 'Tensor' at 0x7f310fa084a0>\n",
      "                'guarded_class': <weakref at 0x7f343b299b80; to 'torch._C._TensorMeta' at 0x62bccb0 (Tensor)>\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': ['TENSOR_MATCH'],\n",
      "                'code': None,\n",
      "                'obj_weakref': <weakref at 0x7f300f1fc860; to 'Tensor' at 0x7f310fa08590>\n",
      "                'guarded_class': <weakref at 0x7f343b299b80; to 'torch._C._TensorMeta' at 0x62bccb0 (Tensor)>\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x : torch.Tensor, y : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_232813/2571593824.py:15, code: a = x * y\n",
      "        mul = x * y;  x = y = None\n",
      "        return (mul,)\n",
      "        \n",
      "opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    x       x                        ()         {}\n",
      "placeholder    y       y                        ()         {}\n",
      "call_function  mul     <built-in function mul>  (x, y)     {}\n",
      "output         output  output                   ((mul,),)  {}\n",
      "break!\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, y : torch.Tensor, a : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_232813/2571593824.py:17, code: b = a * y\n",
      "        mul = a * y;  a = y = None\n",
      "        return (mul,)\n",
      "        \n",
      "opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    y       y                        ()         {}\n",
      "placeholder    a       a                        ()         {}\n",
      "call_function  mul     <built-in function mul>  (a, y)     {}\n",
      "output         output  output                   ((mul,),)  {}\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_232813/2571593824.py:19, code: return a + b\n",
      "        add = a + b;  a = b = None\n",
      "        return (add,)\n",
      "        \n",
      "opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  add     <built-in function add>  (a, b)     {}\n",
      "output         output  output                   ((add,),)  {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.4914e-02, -1.2275e+00,  1.2223e+01,  7.6648e-03, -5.1721e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "import logging\n",
    "from torch import _dynamo\n",
    "\n",
    "torch._dynamo.config.log_level = False\n",
    "torch._dynamo.config.output_code = False\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs= List[torch.Tensor]):\n",
    "    code = gm.print_readable()\n",
    "    gm.graph.print_tabular()\n",
    "    return gm.forward\n",
    "\n",
    "def toy_example(x, y):\n",
    "    a = x * y\n",
    "    print(\"break!\")\n",
    "    b = a * y\n",
    "    torch._dynamo.graph_break()\n",
    "    return a + b\n",
    "\n",
    "compiled_fn = torch.compile(toy_example, backend=my_compiler)\n",
    "\n",
    "compiled_fn(torch.randn(5), torch.randn(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a5d7572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"364pt\" height=\"359pt\" viewBox=\"0.00 0.00 364.00 359.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 355)\">\n",
       "<title>f</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-355 360,-355 360,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M12,-258.5C12,-258.5 157,-258.5 157,-258.5 163,-258.5 169,-264.5 169,-270.5 169,-270.5 169,-338.5 169,-338.5 169,-344.5 163,-350.5 157,-350.5 157,-350.5 12,-350.5 12,-350.5 6,-350.5 0,-344.5 0,-338.5 0,-338.5 0,-270.5 0,-270.5 0,-264.5 6,-258.5 12,-258.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-335.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%x</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-327.5 169,-327.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-304.5 169,-304.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=x</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-281.5 169,-281.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-266.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- mul -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>mul</title>\n",
       "<path fill=\"#eed5d2\" stroke=\"black\" d=\"M101.5,-129.5C101.5,-129.5 253.5,-129.5 253.5,-129.5 259.5,-129.5 265.5,-135.5 265.5,-141.5 265.5,-141.5 265.5,-209.5 265.5,-209.5 265.5,-215.5 259.5,-221.5 253.5,-221.5 253.5,-221.5 101.5,-221.5 101.5,-221.5 95.5,-221.5 89.5,-215.5 89.5,-209.5 89.5,-209.5 89.5,-141.5 89.5,-141.5 89.5,-135.5 95.5,-129.5 101.5,-129.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-206.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%mul</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-198.5 265.5,-198.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-183.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=call_function</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-175.5 265.5,-175.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-160.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=_operator.mul</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-152.5 265.5,-152.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-137.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;mul -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x-&gt;mul</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.55,-258.37C124.25,-249.21 131.36,-239.5 138.26,-230.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.16,-232.05 144.24,-221.92 135.51,-227.92 141.16,-232.05\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>y</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M199,-258.5C199,-258.5 344,-258.5 344,-258.5 350,-258.5 356,-264.5 356,-270.5 356,-270.5 356,-338.5 356,-338.5 356,-344.5 350,-350.5 344,-350.5 344,-350.5 199,-350.5 199,-350.5 193,-350.5 187,-344.5 187,-338.5 187,-338.5 187,-270.5 187,-270.5 187,-264.5 193,-258.5 199,-258.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-335.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%y</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-327.5 356,-327.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-304.5 356,-304.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=y</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-281.5 356,-281.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-266.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- y&#45;&gt;mul -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>y-&gt;mul</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M238.09,-258.37C231.32,-249.21 224.13,-239.5 217.16,-230.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.88,-227.87 211.11,-221.92 214.25,-232.04 219.88,-227.87\"/>\n",
       "</g>\n",
       "<!-- output -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>output</title>\n",
       "<path fill=\"PowderBlue\" stroke=\"black\" d=\"M123,-0.5C123,-0.5 232,-0.5 232,-0.5 238,-0.5 244,-6.5 244,-12.5 244,-12.5 244,-80.5 244,-80.5 244,-86.5 238,-92.5 232,-92.5 232,-92.5 123,-92.5 123,-92.5 117,-92.5 111,-86.5 111,-80.5 111,-80.5 111,-12.5 111,-12.5 111,-6.5 117,-0.5 123,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-77.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-69.5 244,-69.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-46.5 244,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-23.5 244,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=0</text>\n",
       "</g>\n",
       "<!-- mul&#45;&gt;output -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>mul-&gt;output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.5,-129.37C177.5,-120.86 177.5,-111.87 177.5,-103.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181,-102.92 177.5,-92.92 174,-102.92 181,-102.92\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"364pt\" height=\"359pt\" viewBox=\"0.00 0.00 364.00 359.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 355)\">\n",
       "<title>f</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-355 360,-355 360,4 -4,4\"/>\n",
       "<!-- y -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>y</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M12,-258.5C12,-258.5 157,-258.5 157,-258.5 163,-258.5 169,-264.5 169,-270.5 169,-270.5 169,-338.5 169,-338.5 169,-344.5 163,-350.5 157,-350.5 157,-350.5 12,-350.5 12,-350.5 6,-350.5 0,-344.5 0,-338.5 0,-338.5 0,-270.5 0,-270.5 0,-264.5 6,-258.5 12,-258.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-335.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%y</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-327.5 169,-327.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-304.5 169,-304.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=y</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-281.5 169,-281.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-266.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- mul -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>mul</title>\n",
       "<path fill=\"#eed5d2\" stroke=\"black\" d=\"M101.5,-129.5C101.5,-129.5 253.5,-129.5 253.5,-129.5 259.5,-129.5 265.5,-135.5 265.5,-141.5 265.5,-141.5 265.5,-209.5 265.5,-209.5 265.5,-215.5 259.5,-221.5 253.5,-221.5 253.5,-221.5 101.5,-221.5 101.5,-221.5 95.5,-221.5 89.5,-215.5 89.5,-209.5 89.5,-209.5 89.5,-141.5 89.5,-141.5 89.5,-135.5 95.5,-129.5 101.5,-129.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-206.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%mul</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-198.5 265.5,-198.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-183.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=call_function</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-175.5 265.5,-175.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-160.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=_operator.mul</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-152.5 265.5,-152.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-137.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- y&#45;&gt;mul -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>y-&gt;mul</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.55,-258.37C124.25,-249.21 131.36,-239.5 138.26,-230.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.16,-232.05 144.24,-221.92 135.51,-227.92 141.16,-232.05\"/>\n",
       "</g>\n",
       "<!-- a -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>a</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M199,-258.5C199,-258.5 344,-258.5 344,-258.5 350,-258.5 356,-264.5 356,-270.5 356,-270.5 356,-338.5 356,-338.5 356,-344.5 350,-350.5 344,-350.5 344,-350.5 199,-350.5 199,-350.5 193,-350.5 187,-344.5 187,-338.5 187,-338.5 187,-270.5 187,-270.5 187,-264.5 193,-258.5 199,-258.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-335.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%a</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-327.5 356,-327.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-304.5 356,-304.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=a</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-281.5 356,-281.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-266.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;mul -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a-&gt;mul</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M238.09,-258.37C231.32,-249.21 224.13,-239.5 217.16,-230.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.88,-227.87 211.11,-221.92 214.25,-232.04 219.88,-227.87\"/>\n",
       "</g>\n",
       "<!-- output -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>output</title>\n",
       "<path fill=\"PowderBlue\" stroke=\"black\" d=\"M123,-0.5C123,-0.5 232,-0.5 232,-0.5 238,-0.5 244,-6.5 244,-12.5 244,-12.5 244,-80.5 244,-80.5 244,-86.5 238,-92.5 232,-92.5 232,-92.5 123,-92.5 123,-92.5 117,-92.5 111,-86.5 111,-80.5 111,-80.5 111,-12.5 111,-12.5 111,-6.5 117,-0.5 123,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-77.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-69.5 244,-69.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-46.5 244,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-23.5 244,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=0</text>\n",
       "</g>\n",
       "<!-- mul&#45;&gt;output -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>mul-&gt;output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.5,-129.37C177.5,-120.86 177.5,-111.87 177.5,-103.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181,-102.92 177.5,-92.92 174,-102.92 181,-102.92\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"364pt\" height=\"359pt\" viewBox=\"0.00 0.00 364.00 359.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 355)\">\n",
       "<title>f</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-355 360,-355 360,4 -4,4\"/>\n",
       "<!-- a -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>a</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M12,-258.5C12,-258.5 157,-258.5 157,-258.5 163,-258.5 169,-264.5 169,-270.5 169,-270.5 169,-338.5 169,-338.5 169,-344.5 163,-350.5 157,-350.5 157,-350.5 12,-350.5 12,-350.5 6,-350.5 0,-344.5 0,-338.5 0,-338.5 0,-270.5 0,-270.5 0,-264.5 6,-258.5 12,-258.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-335.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%a</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-327.5 169,-327.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-304.5 169,-304.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=a</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-281.5 169,-281.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-266.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- add -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>add</title>\n",
       "<path fill=\"#ffe7ba\" stroke=\"black\" d=\"M101.5,-129.5C101.5,-129.5 253.5,-129.5 253.5,-129.5 259.5,-129.5 265.5,-135.5 265.5,-141.5 265.5,-141.5 265.5,-209.5 265.5,-209.5 265.5,-215.5 259.5,-221.5 253.5,-221.5 253.5,-221.5 101.5,-221.5 101.5,-221.5 95.5,-221.5 89.5,-215.5 89.5,-209.5 89.5,-209.5 89.5,-141.5 89.5,-141.5 89.5,-135.5 95.5,-129.5 101.5,-129.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-206.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%add</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-198.5 265.5,-198.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-183.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=call_function</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-175.5 265.5,-175.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-160.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=_operator.add</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"89.5,-152.5 265.5,-152.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-137.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;add -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>a-&gt;add</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.55,-258.37C124.25,-249.21 131.36,-239.5 138.26,-230.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.16,-232.05 144.24,-221.92 135.51,-227.92 141.16,-232.05\"/>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>b</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M199,-258.5C199,-258.5 344,-258.5 344,-258.5 350,-258.5 356,-264.5 356,-270.5 356,-270.5 356,-338.5 356,-338.5 356,-344.5 350,-350.5 344,-350.5 344,-350.5 199,-350.5 199,-350.5 193,-350.5 187,-344.5 187,-338.5 187,-338.5 187,-270.5 187,-270.5 187,-264.5 193,-258.5 199,-258.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-335.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%b</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-327.5 356,-327.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-304.5 356,-304.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=b</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-281.5 356,-281.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-266.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;add -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>b-&gt;add</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M238.09,-258.37C231.32,-249.21 224.13,-239.5 217.16,-230.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.88,-227.87 211.11,-221.92 214.25,-232.04 219.88,-227.87\"/>\n",
       "</g>\n",
       "<!-- output -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>output</title>\n",
       "<path fill=\"PowderBlue\" stroke=\"black\" d=\"M123,-0.5C123,-0.5 232,-0.5 232,-0.5 238,-0.5 244,-6.5 244,-12.5 244,-12.5 244,-80.5 244,-80.5 244,-86.5 238,-92.5 232,-92.5 232,-92.5 123,-92.5 123,-92.5 117,-92.5 111,-86.5 111,-80.5 111,-80.5 111,-12.5 111,-12.5 111,-6.5 117,-0.5 123,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-77.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-69.5 244,-69.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-46.5 244,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111,-23.5 244,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=0</text>\n",
       "</g>\n",
       "<!-- add&#45;&gt;output -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>add-&gt;output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.5,-129.37C177.5,-120.86 177.5,-111.87 177.5,-103.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181,-102.92 177.5,-92.92 174,-102.92 181,-102.92\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"368pt\" height=\"617pt\" viewBox=\"0.00 0.00 367.50 617.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 613)\">\n",
       "<title>f</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-613 363.5,-613 363.5,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M12,-516.5C12,-516.5 157,-516.5 157,-516.5 163,-516.5 169,-522.5 169,-528.5 169,-528.5 169,-596.5 169,-596.5 169,-602.5 163,-608.5 157,-608.5 157,-608.5 12,-608.5 12,-608.5 6,-608.5 0,-602.5 0,-596.5 0,-596.5 0,-528.5 0,-528.5 0,-522.5 6,-516.5 12,-516.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-593.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%x</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-585.5 169,-585.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-570.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-562.5 169,-562.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-547.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=x</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-539.5 169,-539.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-524.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- mul -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>mul</title>\n",
       "<path fill=\"#eed5d2\" stroke=\"black\" d=\"M79.5,-387.5C79.5,-387.5 231.5,-387.5 231.5,-387.5 237.5,-387.5 243.5,-393.5 243.5,-399.5 243.5,-399.5 243.5,-467.5 243.5,-467.5 243.5,-473.5 237.5,-479.5 231.5,-479.5 231.5,-479.5 79.5,-479.5 79.5,-479.5 73.5,-479.5 67.5,-473.5 67.5,-467.5 67.5,-467.5 67.5,-399.5 67.5,-399.5 67.5,-393.5 73.5,-387.5 79.5,-387.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-464.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%mul</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"67.5,-456.5 243.5,-456.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-441.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=call_function</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"67.5,-433.5 243.5,-433.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-418.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=_operator.mul</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"67.5,-410.5 243.5,-410.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-395.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=2</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;mul -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x-&gt;mul</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M109.73,-516.37C114.75,-507.4 120.06,-497.89 125.22,-488.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"128.29,-490.35 130.11,-479.92 122.18,-486.94 128.29,-490.35\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>y</title>\n",
       "<path fill=\"AliceBlue\" stroke=\"black\" d=\"M199,-516.5C199,-516.5 344,-516.5 344,-516.5 350,-516.5 356,-522.5 356,-528.5 356,-528.5 356,-596.5 356,-596.5 356,-602.5 350,-608.5 344,-608.5 344,-608.5 199,-608.5 199,-608.5 193,-608.5 187,-602.5 187,-596.5 187,-596.5 187,-528.5 187,-528.5 187,-522.5 193,-516.5 199,-516.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-593.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%y</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-585.5 356,-585.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-570.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=placeholder</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-562.5 356,-562.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-547.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=y</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"187,-539.5 356,-539.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-524.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=2</text>\n",
       "</g>\n",
       "<!-- y&#45;&gt;mul -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>y-&gt;mul</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.28,-516.37C221.75,-507.03 212.69,-497.11 203.93,-487.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.31,-484.94 196.98,-479.92 201.14,-489.66 206.31,-484.94\"/>\n",
       "</g>\n",
       "<!-- mul_1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>mul_1</title>\n",
       "<path fill=\"#eed5d2\" stroke=\"black\" d=\"M195.5,-258.5C195.5,-258.5 347.5,-258.5 347.5,-258.5 353.5,-258.5 359.5,-264.5 359.5,-270.5 359.5,-270.5 359.5,-338.5 359.5,-338.5 359.5,-344.5 353.5,-350.5 347.5,-350.5 347.5,-350.5 195.5,-350.5 195.5,-350.5 189.5,-350.5 183.5,-344.5 183.5,-338.5 183.5,-338.5 183.5,-270.5 183.5,-270.5 183.5,-264.5 189.5,-258.5 195.5,-258.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-335.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%mul_1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"183.5,-327.5 359.5,-327.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=call_function</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"183.5,-304.5 359.5,-304.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=_operator.mul</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"183.5,-281.5 359.5,-281.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-266.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- y&#45;&gt;mul_1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>y-&gt;mul_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M271.5,-516.32C271.5,-472.95 271.5,-407.35 271.5,-360.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275,-360.66 271.5,-350.66 268,-360.66 275,-360.66\"/>\n",
       "</g>\n",
       "<!-- mul&#45;&gt;mul_1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>mul-&gt;mul_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.72,-387.37C205.25,-378.03 214.31,-368.11 223.07,-358.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"225.86,-360.66 230.02,-350.92 220.69,-355.94 225.86,-360.66\"/>\n",
       "</g>\n",
       "<!-- add -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>add</title>\n",
       "<path fill=\"#ffe7ba\" stroke=\"black\" d=\"M137.5,-129.5C137.5,-129.5 289.5,-129.5 289.5,-129.5 295.5,-129.5 301.5,-135.5 301.5,-141.5 301.5,-141.5 301.5,-209.5 301.5,-209.5 301.5,-215.5 295.5,-221.5 289.5,-221.5 289.5,-221.5 137.5,-221.5 137.5,-221.5 131.5,-221.5 125.5,-215.5 125.5,-209.5 125.5,-209.5 125.5,-141.5 125.5,-141.5 125.5,-135.5 131.5,-129.5 137.5,-129.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-206.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%add</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"125.5,-198.5 301.5,-198.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-183.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=call_function</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"125.5,-175.5 301.5,-175.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-160.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=_operator.add</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"125.5,-152.5 301.5,-152.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-137.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=1</text>\n",
       "</g>\n",
       "<!-- mul&#45;&gt;add -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>mul-&gt;add</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.26,-387.23C157.85,-351.53 162.32,-300.79 174.5,-258 177.08,-248.93 180.6,-239.63 184.48,-230.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.68,-232.11 188.62,-221.55 181.31,-229.22 187.68,-232.11\"/>\n",
       "</g>\n",
       "<!-- mul_1&#45;&gt;add -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>mul_1-&gt;add</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M250.89,-258.37C246.88,-249.58 242.63,-240.29 238.5,-231.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"241.58,-229.56 234.24,-221.92 235.21,-232.47 241.58,-229.56\"/>\n",
       "</g>\n",
       "<!-- output -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>output</title>\n",
       "<path fill=\"PowderBlue\" stroke=\"black\" d=\"M159,-0.5C159,-0.5 268,-0.5 268,-0.5 274,-0.5 280,-6.5 280,-12.5 280,-12.5 280,-80.5 280,-80.5 280,-86.5 274,-92.5 268,-92.5 268,-92.5 159,-92.5 159,-92.5 153,-92.5 147,-86.5 147,-80.5 147,-80.5 147,-12.5 147,-12.5 147,-6.5 153,-0.5 159,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-77.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">name=%output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"147,-69.5 280,-69.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op_code=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"147,-46.5 280,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target=output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"147,-23.5 280,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_users=0</text>\n",
       "</g>\n",
       "<!-- add&#45;&gt;output -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>add-&gt;output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.5,-129.37C213.5,-120.86 213.5,-111.87 213.5,-103.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217,-102.92 213.5,-92.92 210,-102.92 217,-102.92\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "# Display the SVG image\n",
    "display(SVG(filename='graph_a.svg'))\n",
    "display(SVG(filename='graph_b.svg'))\n",
    "display(SVG(filename='graph_c.svg'))\n",
    "\n",
    "display(SVG(filename=\"forward.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc7e55",
   "metadata": {},
   "source": [
    "### AOT/IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb6b248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamo produced an fx Graph in Torch IR:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, s0 : torch.SymInt, L_x_ : torch.Tensor):\n",
      "        l_x_ = L_x_\n",
      "        \n",
      "        # File: /tmp/ipykernel_395751/2829938787.py:12, code: x = self.fc1(x)\n",
      "        l__self___fc1 = self.L__self___fc1(l_x_);  l_x_ = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_395751/2829938787.py:13, code: x = torch.nn.functional.gelu(x)\n",
      "        gelu = torch._C._nn.gelu(l__self___fc1);  l__self___fc1 = None\n",
      "        return (gelu,)\n",
      "        \n",
      "[s0, tensor([[-2.0535, -0.6647, -0.0672, -0.7256,  1.2605, -0.3283, -0.5183,  0.3574,\n",
      "         -0.0907,  0.0303, -1.2413, -0.3939, -1.2627, -0.0352, -0.2364, -1.1007,\n",
      "         -0.0522,  0.5258,  0.7490,  0.7043,  0.8335,  0.0896,  2.1802,  0.0320,\n",
      "          0.5655, -1.2451, -0.8018, -0.7264, -0.3776, -0.0621, -1.3117, -1.4410],\n",
      "        [-0.7965, -1.0759, -0.0176,  0.1857, -1.3831, -0.5108, -0.2139, -1.6573,\n",
      "          0.5811,  1.1196, -0.8478,  1.3801, -0.6072,  0.4214, -0.0224, -1.0927,\n",
      "          0.6514, -0.6585,  1.0548,  0.5289,  1.3342,  1.0547,  0.2736, -0.5645,\n",
      "         -0.4529,  0.6390,  0.8643,  0.4694, -0.8472, -4.0208,  0.7929,  0.4838],\n",
      "        [ 1.1156,  0.3501, -0.9081, -0.0277, -1.4181, -1.7783,  0.2264, -0.5358,\n",
      "          0.5109,  0.3662, -0.2773, -1.0979, -0.4756,  0.5962,  0.3546, -0.9543,\n",
      "          1.2428,  0.7431, -1.5113, -1.1955, -1.4774, -0.4478, -0.2229, -0.2548,\n",
      "          0.1732, -0.1567, -0.5811,  1.4971, -1.8172,  0.0114,  0.7992,  0.0348],\n",
      "        [ 0.5384, -0.1658,  2.5083, -0.0780,  0.5945,  0.0204, -1.0087,  0.8472,\n",
      "         -1.9584, -1.1466,  1.9118,  1.1522,  0.2651,  0.0202, -0.2151, -0.4896,\n",
      "         -1.5551,  0.1431, -0.0945, -1.4778, -0.8814, -0.8863,  0.8567,  0.2494,\n",
      "         -1.3514, -0.3979, -0.8137,  1.3401,  0.6679,  0.8638,  0.3157, -0.1501],\n",
      "        [ 1.6612,  0.0363,  0.5182,  0.3625,  0.2313, -0.5525, -0.0404, -1.5995,\n",
      "          0.4162, -0.1898,  1.1270,  1.6690,  1.6457,  1.4937, -0.6934, -0.2148,\n",
      "          1.0798,  0.2435, -2.5493,  0.3700, -0.4695,  1.1977, -1.0174,  1.2631,\n",
      "         -0.7970, -3.3461, -0.9969,  0.4152,  2.2740,  1.6850, -1.6909,  0.3196],\n",
      "        [-1.9552,  1.8596,  0.9757,  0.8764,  1.0589, -2.1208, -1.1026,  1.0488,\n",
      "          0.7814, -0.8729,  0.3563,  1.1595, -1.1023, -0.2802,  0.7359, -0.1427,\n",
      "          0.2429, -1.0208,  0.4248,  0.5352, -0.3592,  0.9018, -0.8611,  0.2904,\n",
      "          0.9748,  0.0674, -1.6773,  1.2763,  0.6538, -1.1530, -1.6105,  0.4155],\n",
      "        [-0.8141, -2.1721, -0.3113, -0.0194, -0.1667,  1.5184, -1.3813,  0.9439,\n",
      "         -1.2987, -0.5848, -1.2109, -0.6943,  0.0395, -0.9829, -0.7395, -0.7453,\n",
      "          1.0617, -0.4323,  1.2023, -2.7055,  1.1162, -1.0163,  0.1790,  0.7299,\n",
      "          0.6043, -0.9417,  1.5989, -1.2349,  0.1744, -1.0453, -0.3041, -1.2878],\n",
      "        [ 1.1021,  0.5385,  0.1346,  0.6056, -0.6197, -0.6020,  0.4871, -0.7974,\n",
      "          1.6492, -0.8166, -0.0614,  1.8864,  1.3460, -0.3041, -0.3007,  1.2891,\n",
      "         -1.2010, -0.1896, -0.4008,  0.0830,  1.9014,  0.0865, -0.4138,  0.5548,\n",
      "         -0.5523, -0.8611,  0.9998,  0.4653,  1.2548, -0.9816,  0.0578, -1.1712]])]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch._dynamo\n",
    "\n",
    "### Simple module:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        return x\n",
    "    \n",
    "model = MLP()\n",
    "bs = 8\n",
    "input = torch.randn(bs, 32)\n",
    "\n",
    "### Custom compiler\n",
    "def toy_compiler(gm, sample_inputs):\n",
    "    print(\"Dynamo produced an fx Graph in Torch IR:\")\n",
    "    gm.print_readable()\n",
    "    print(sample_inputs)\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "fn = torch.compile(backend=toy_compiler, dynamic=True)(model)\n",
    "\n",
    "# Trigger compilation on forwards on first run\n",
    "out = fn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1db0dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOTAutograd produced and fx graph in ATen IR\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: Sym(s0), primals_4: f32[s0, 32]):\n",
      "        # File: /tmp/ipykernel_395751/2829938787.py:12, code: x = self.fc1(x)\n",
      "        t: f32[32, 64] = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "        addmm: f32[s0, 64] = torch.ops.aten.addmm.default(primals_2, primals_4, t);  primals_2 = t = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_395751/2829938787.py:13, code: x = torch.nn.functional.gelu(x)\n",
      "        gelu: f32[s0, 64] = torch.ops.aten.gelu.default(addmm)\n",
      "        return [gelu, primals_4, addmm, primals_3]\n",
      "        \n",
      "[Parameter(FakeTensor(..., size=(64, 32), requires_grad=True)), Parameter(FakeTensor(..., size=(64,), requires_grad=True)), s0, FakeTensor(..., size=(s0, 32))]\n",
      "AOTAutograd produced and fx graph in ATen IR\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_3: Sym(s0), primals_4: f32[s0, 32], addmm: f32[s0, 64], tangents_1: f32[s0, 64]):\n",
      "        # File: /tmp/ipykernel_395751/2829938787.py:13, code: x = torch.nn.functional.gelu(x)\n",
      "        gelu_backward: f32[s0, 64] = torch.ops.aten.gelu_backward.default(tangents_1, addmm);  tangents_1 = addmm = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_395751/2829938787.py:12, code: x = self.fc1(x)\n",
      "        t_1: f32[64, s0] = torch.ops.aten.t.default(gelu_backward)\n",
      "        mm: f32[64, 32] = torch.ops.aten.mm.default(t_1, primals_4);  t_1 = primals_4 = None\n",
      "        t_2: f32[32, 64] = torch.ops.aten.t.default(mm);  mm = None\n",
      "        sum_1: f32[1, 64] = torch.ops.aten.sum.dim_IntList(gelu_backward, [0], True);  gelu_backward = None\n",
      "        view: f32[64] = torch.ops.aten.view.default(sum_1, [64]);  sum_1 = None\n",
      "        t_3: f32[64, 32] = torch.ops.aten.t.default(t_2);  t_2 = None\n",
      "        return [t_3, view, None, None]\n",
      "        \n",
      "[s0, FakeTensor(..., size=(s0, 32)), FakeTensor(..., size=(s0, 64)), FakeTensor(..., size=(s0, 64))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:1510: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Invoke AOTAutograd\n",
    "# - Captures fwd/bwd graph\n",
    "# - Required for training\n",
    "# - Lower from torch level are to ATen IR\n",
    "\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "def new_backend(gm, sample_inputs):\n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        print(\"AOTAutograd produced and fx graph in ATen IR\")\n",
    "        gm.print_readable()\n",
    "        print(sample_inputs)\n",
    "        return gm.forward\n",
    "    \n",
    "    return aot_module_simplified(\n",
    "        gm, \n",
    "        sample_inputs,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# new bakcned invokes aot_autograd explicitly\n",
    "fn = torch.compile(backend=new_backend, dynamic=True)(model)\n",
    "\n",
    "out = fn(input)\n",
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2240eee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<OpOverload(op='aten.addcdiv', overload='default')>: <function addcdiv at 0x7f2a254f01f0>, <OpOverload(op='aten.addcdiv', overload='out')>: <function addcdiv at 0x7f2a254f01f0>, <OpOverload(op='aten.addcdiv_', overload='default')>: <function addcdiv at 0x7f2a254dd670>, <OpOverload(op='aten.addcmul', overload='default')>: <function addcmul at 0x7f2a254f03a0>, <OpOverload(op='aten.addcmul', overload='out')>: <function addcmul at 0x7f2a254f03a0>, <OpOverload(op='aten.addcmul_', overload='default')>: <function addcmul at 0x7f2a254dd5e0>, <OpOverload(op='aten.addr', overload='default')>: <function addr at 0x7f2a254fee50>, <OpOverload(op='aten.addr', overload='out')>: <function addr at 0x7f2a254fee50>, <OpOverload(op='aten.affine_grid_generator', overload='default')>: <function affine_grid_generator at 0x7f2a25695940>, <OpOverload(op='aten.affine_grid_generator', overload='out')>: <function affine_grid_generator at 0x7f2a25695940>, <OpOverload(op='aten.aminmax', overload='default')>: <function aminmax at 0x7f2a2569f1f0>, <OpOverload(op='aten.aminmax', overload='out')>: <function aminmax at 0x7f2a2569f1f0>, <OpOverload(op='aten.arange', overload='default')>: <function arange_default at 0x7f2a2569fb80>, <OpOverload(op='aten.arange', overload='start')>: <function arange_start at 0x7f2a2569f940>, <OpOverload(op='aten.binary_cross_entropy', overload='default')>: <function binary_cross_entropy at 0x7f2a256d1310>, <OpOverload(op='aten.binary_cross_entropy', overload='out')>: <function binary_cross_entropy at 0x7f2a256d1310>, <OpOverload(op='aten.binary_cross_entropy_backward', overload='default')>: <function binary_cross_entropy_backward at 0x7f2a256d1430>, <OpOverload(op='aten.binary_cross_entropy_backward', overload='grad_input')>: <function binary_cross_entropy_backward at 0x7f2a256d1430>, <OpOverload(op='aten.binary_cross_entropy_with_logits', overload='default')>: <function binary_cross_entropy_with_logits at 0x7f2a25695af0>, <OpOverload(op='aten.binary_cross_entropy_with_logits', overload='out')>: <function binary_cross_entropy_with_logits at 0x7f2a25695af0>, <OpOverload(op='aten.celu', overload='default')>: <function celu at 0x7f2a25456430>, <OpOverload(op='aten.celu', overload='out')>: <function celu at 0x7f2a25456430>, <OpOverload(op='aten.celu_', overload='default')>: <function celu at 0x7f2a253f80d0>, <OpOverload(op='aten.col2im', overload='default')>: <function col2im at 0x7f2a256de1f0>, <OpOverload(op='aten.col2im', overload='out')>: <function col2im at 0x7f2a256de1f0>, <OpOverload(op='aten.count_nonzero', overload='dim_IntList')>: <function count_nonzero at 0x7f2a254dd160>, <OpOverload(op='aten.count_nonzero', overload='dim_IntList_out')>: <function count_nonzero at 0x7f2a254dd160>, <OpOverload(op='aten.count_nonzero', overload='default')>: <function count_nonzero at 0x7f2a254dd160>, <OpOverload(op='aten.count_nonzero', overload='out')>: <function count_nonzero at 0x7f2a254dd160>, <OpOverload(op='aten.cudnn_batch_norm', overload='default')>: <function cudnn_batch_norm at 0x7f2a2566f9d0>, <OpOverload(op='aten.cudnn_batch_norm', overload='out')>: <function cudnn_batch_norm at 0x7f2a2566f9d0>, <OpOverload(op='aten.cudnn_batch_norm_backward', overload='default')>: <function cudnn_batch_norm_backward at 0x7f2a2566faf0>, <OpOverload(op='aten.cudnn_batch_norm_backward', overload='out')>: <function cudnn_batch_norm_backward at 0x7f2a2566faf0>, <OpOverload(op='aten.deg2rad', overload='default')>: <function deg2rad at 0x7f2a254dd3a0>, <OpOverload(op='aten.deg2rad', overload='out')>: <function deg2rad at 0x7f2a254dd3a0>, <OpOverload(op='aten.deg2rad_', overload='default')>: <function deg2rad at 0x7f2a2546c310>, <OpOverload(op='aten.detach', overload='default')>: <function nop_decomposition at 0x7f2a2566f8b0>, <OpOverload(op='aten.diag_embed', overload='default')>: <function diag_embed at 0x7f2a2590a9d0>, <OpOverload(op='aten.diag_embed', overload='out')>: <function diag_embed at 0x7f2a2590a9d0>, <OpOverload(op='aten.diagonal_backward', overload='default')>: <function diagonal_backward at 0x7f2a256d1a60>, <OpOverload(op='aten.diagonal_backward', overload='out')>: <function diagonal_backward at 0x7f2a256d1a60>, <OpOverload(op='aten.dot', overload='default')>: <function dot at 0x7f2a25695e50>, <OpOverload(op='aten.dot', overload='out')>: <function dot at 0x7f2a25695e50>, <OpOverload(op='aten.elu', overload='default')>: <function elu at 0x7f2a25456820>, <OpOverload(op='aten.elu', overload='out')>: <function elu at 0x7f2a25456820>, <OpOverload(op='aten.elu_', overload='default')>: <function elu at 0x7f2a253f8430>, <OpOverload(op='aten.elu_backward', overload='default')>: <function elu_backward at 0x7f2a256b53a0>, <OpOverload(op='aten.elu_backward', overload='grad_input')>: <function elu_backward at 0x7f2a256b53a0>, <OpOverload(op='aten.embedding_dense_backward', overload='default')>: <function embedding_dense_backward at 0x7f2a256de8b0>, <OpOverload(op='aten.embedding_dense_backward', overload='out')>: <function embedding_dense_backward at 0x7f2a256de8b0>, <OpOverload(op='aten.empty_like', overload='default')>: <function empty_like at 0x7f2a254aeaf0>, <OpOverload(op='aten.empty_like', overload='out')>: <function empty_like at 0x7f2a254aeaf0>, <OpOverload(op='aten._euclidean_dist', overload='default')>: <function _euclidean_dist at 0x7f2a256d1820>, <OpOverload(op='aten.eye', overload='default')>: <function eye at 0x7f2a254bb3a0>, <OpOverload(op='aten.eye', overload='m')>: <function eye at 0x7f2a254bb3a0>, <OpOverload(op='aten.eye', overload='out')>: <function eye at 0x7f2a254bb3a0>, <OpOverload(op='aten.eye', overload='m_out')>: <function eye at 0x7f2a254bb3a0>, <OpOverload(op='aten.fill', overload='Scalar')>: <function fill_scalar at 0x7f2a256b5430>, <OpOverload(op='aten.fill', overload='Tensor')>: <function fill_tensor at 0x7f2a256b54c0>, <OpOverload(op='aten.fill_', overload='Scalar')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562b670>, <OpOverload(op='aten.fill_', overload='Tensor')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562b670>, <OpOverload(op='aten.frac', overload='default')>: <function frac at 0x7f2a255b5940>, <OpOverload(op='aten.frac', overload='out')>: <function frac at 0x7f2a255b5940>, <OpOverload(op='aten.frac_', overload='default')>: <function frac at 0x7f2a2546caf0>, <OpOverload(op='aten.gelu_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562b700>, <OpOverload(op='aten.gelu_backward', overload='default')>: <function gelu_backward at 0x7f2a256b5ca0>, <OpOverload(op='aten.gelu_backward', overload='grad_input')>: <function gelu_backward at 0x7f2a256b5ca0>, <OpOverload(op='aten.glu_backward', overload='default')>: <function glu_backward at 0x7f2a256d10d0>, <OpOverload(op='aten.glu_backward', overload='grad_input')>: <function glu_backward at 0x7f2a256d10d0>, <OpOverload(op='aten.grid_sampler_2d', overload='default')>: <function grid_sampler_2d at 0x7f2a25695a60>, <OpOverload(op='aten.grid_sampler_2d', overload='out')>: <function grid_sampler_2d at 0x7f2a25695a60>, <OpOverload(op='aten.hardshrink', overload='default')>: <function hardshrink at 0x7f2a254619d0>, <OpOverload(op='aten.hardshrink', overload='out')>: <function hardshrink at 0x7f2a254619d0>, <OpOverload(op='aten.hardsigmoid', overload='default')>: <function hardsigmoid at 0x7f2a256b55e0>, <OpOverload(op='aten.hardsigmoid', overload='out')>: <function hardsigmoid at 0x7f2a256b55e0>, <OpOverload(op='aten.hardsigmoid_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562b8b0>, <OpOverload(op='aten.hardsigmoid_backward', overload='default')>: <function hardsigmoid_backward at 0x7f2a256b5700>, <OpOverload(op='aten.hardsigmoid_backward', overload='grad_input')>: <function hardsigmoid_backward at 0x7f2a256b5700>, <OpOverload(op='aten.hardswish', overload='default')>: <function hardswish at 0x7f2a256b58b0>, <OpOverload(op='aten.hardswish', overload='out')>: <function hardswish at 0x7f2a256b58b0>, <OpOverload(op='aten.hardswish_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562b790>, <OpOverload(op='aten.hardswish_backward', overload='default')>: <function hardswish_backward at 0x7f2a256b59d0>, <OpOverload(op='aten.hardswish_backward', overload='out')>: <function hardswish_backward at 0x7f2a256b59d0>, <OpOverload(op='aten.hardtanh_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562b820>, <OpOverload(op='aten.hardtanh_backward', overload='default')>: <function hardtanh_backward at 0x7f2a256b5790>, <OpOverload(op='aten.hardtanh_backward', overload='grad_input')>: <function hardtanh_backward at 0x7f2a256b5790>, <OpOverload(op='aten.heaviside', overload='default')>: <function heaviside at 0x7f2a25528ca0>, <OpOverload(op='aten.heaviside', overload='out')>: <function heaviside at 0x7f2a25528ca0>, <OpOverload(op='aten.heaviside_', overload='default')>: <function heaviside at 0x7f2a2546cd30>, <OpOverload(op='aten.huber_loss', overload='default')>: <function huber_loss at 0x7f2a253ed5e0>, <OpOverload(op='aten.huber_loss', overload='out')>: <function huber_loss at 0x7f2a253ed5e0>, <OpOverload(op='aten.huber_loss_backward', overload='default')>: <function huber_loss_backward at 0x7f2a256c6dc0>, <OpOverload(op='aten.huber_loss_backward', overload='out')>: <function huber_loss_backward_out at 0x7f2a256c6ee0>, <OpOverload(op='aten.im2col', overload='default')>: <function im2col at 0x7f2a256de040>, <OpOverload(op='aten.im2col', overload='out')>: <function im2col at 0x7f2a256de040>, <OpOverload(op='aten.index_add', overload='default')>: <function index_add at 0x7f2a2566fe50>, <OpOverload(op='aten.index_add', overload='out')>: <function index_add at 0x7f2a2566fe50>, <OpOverload(op='aten.index_add', overload='dimname')>: <function index_add at 0x7f2a2566fe50>, <OpOverload(op='aten.index_add_', overload='default')>: <function index_add_ at 0x7f2a2566fca0>, <OpOverload(op='aten.index_copy', overload='default')>: <function index_copy at 0x7f2a256840d0>, <OpOverload(op='aten.index_copy', overload='dimname')>: <function index_copy at 0x7f2a256840d0>, <OpOverload(op='aten.index_copy', overload='out')>: <function index_copy at 0x7f2a256840d0>, <OpOverload(op='aten.index_copy_', overload='default')>: <function index_copy_ at 0x7f2a2566fee0>, <OpOverload(op='aten.index_copy_', overload='dimname')>: <function index_copy_ at 0x7f2a2566fee0>, <OpOverload(op='aten.index_fill', overload='int_Tensor')>: <function index_fill at 0x7f2a25a134c0>, <OpOverload(op='aten.index_fill', overload='int_Scalar')>: <function index_fill at 0x7f2a25a134c0>, <OpOverload(op='aten.index_fill', overload='Dimname_Scalar')>: <function index_fill at 0x7f2a25a134c0>, <OpOverload(op='aten.index_fill', overload='Dimname_Tensor')>: <function index_fill at 0x7f2a25a134c0>, <OpOverload(op='aten.index_fill', overload='int_Scalar_out')>: <function index_fill at 0x7f2a25a134c0>, <OpOverload(op='aten.index_fill', overload='int_Tensor_out')>: <function index_fill at 0x7f2a25a134c0>, <OpOverload(op='aten.index_fill_', overload='int_Tensor')>: <function index_fill_ at 0x7f2a25a133a0>, <OpOverload(op='aten.index_fill_', overload='int_Scalar')>: <function index_fill_ at 0x7f2a25a133a0>, <OpOverload(op='aten.index_fill_', overload='Dimname_Scalar')>: <function index_fill_ at 0x7f2a25a133a0>, <OpOverload(op='aten.index_fill_', overload='Dimname_Tensor')>: <function index_fill_ at 0x7f2a25a133a0>, <OpOverload(op='aten.isneginf', overload='default')>: <function isneginf at 0x7f2a255bf550>, <OpOverload(op='aten.isneginf', overload='out')>: <function isneginf at 0x7f2a255bf550>, <OpOverload(op='aten.isposinf', overload='default')>: <function isposinf at 0x7f2a255bf280>, <OpOverload(op='aten.isposinf', overload='out')>: <function isposinf at 0x7f2a255bf280>, <OpOverload(op='aten.leaky_relu_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562bd30>, <OpOverload(op='aten.leaky_relu_backward', overload='default')>: <function leaky_relu_backward at 0x7f2a256b5b80>, <OpOverload(op='aten.leaky_relu_backward', overload='grad_input')>: <function leaky_relu_backward at 0x7f2a256b5b80>, <OpOverload(op='aten.lerp', overload='Scalar')>: <function lerp at 0x7f2a254aee50>, <OpOverload(op='aten.lerp', overload='Tensor')>: <function lerp at 0x7f2a254aee50>, <OpOverload(op='aten.lerp', overload='Scalar_out')>: <function lerp at 0x7f2a254aee50>, <OpOverload(op='aten.lerp', overload='Tensor_out')>: <function lerp at 0x7f2a254aee50>, <OpOverload(op='aten.lerp_', overload='Scalar')>: <function lerp at 0x7f2a2547e160>, <OpOverload(op='aten.lerp_', overload='Tensor')>: <function lerp at 0x7f2a2547e160>, <OpOverload(op='aten.linspace', overload='default')>: <function linspace at 0x7f2a254aef70>, <OpOverload(op='aten.linspace', overload='out')>: <function linspace at 0x7f2a254aef70>, <OpOverload(op='aten.logaddexp', overload='default')>: <function logaddexp at 0x7f2a2553ab80>, <OpOverload(op='aten.logaddexp', overload='out')>: <function logaddexp at 0x7f2a2553ab80>, <OpOverload(op='aten.logaddexp2', overload='default')>: <function logaddexp2 at 0x7f2a2553adc0>, <OpOverload(op='aten.logaddexp2', overload='out')>: <function logaddexp2 at 0x7f2a2553adc0>, <OpOverload(op='aten.logit', overload='default')>: <function logit at 0x7f2a25411310>, <OpOverload(op='aten.logit', overload='out')>: <function logit at 0x7f2a25411310>, <OpOverload(op='aten.logit_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562bdc0>, <OpOverload(op='aten.logit_backward', overload='default')>: <function logit_backward at 0x7f2a256de3a0>, <OpOverload(op='aten.log_sigmoid_backward', overload='default')>: <function log_sigmoid_backward at 0x7f2a256c65e0>, <OpOverload(op='aten.log_sigmoid_backward', overload='grad_input')>: <function log_sigmoid_backward at 0x7f2a256c65e0>, <OpOverload(op='aten.log_sigmoid_forward', overload='default')>: <function log_sigmoid_forward at 0x7f2a25684160>, <OpOverload(op='aten.log_sigmoid_forward', overload='output')>: <function log_sigmoid_forward at 0x7f2a25684160>, <OpOverload(op='aten._log_softmax_backward_data', overload='default')>: <function _log_softmax_backward_data at 0x7f2a256d1d30>, <OpOverload(op='aten._log_softmax_backward_data', overload='out')>: <function _log_softmax_backward_data at 0x7f2a256d1d30>, <OpOverload(op='aten.logspace', overload='default')>: <function logspace at 0x7f2a254bb0d0>, <OpOverload(op='aten.logspace', overload='out')>: <function logspace at 0x7f2a254bb0d0>, <OpOverload(op='aten.logsumexp', overload='default')>: <function logsumexp at 0x7f2a255caf70>, <OpOverload(op='aten.masked_fill', overload='Scalar')>: <function masked_fill at 0x7f2a254bb8b0>, <OpOverload(op='aten.masked_fill', overload='Tensor')>: <function masked_fill at 0x7f2a254bb8b0>, <OpOverload(op='aten.masked_fill', overload='Scalar_out')>: <function masked_fill at 0x7f2a254bb8b0>, <OpOverload(op='aten.masked_fill', overload='Tensor_out')>: <function masked_fill at 0x7f2a254bb8b0>, <OpOverload(op='aten.masked_fill_', overload='Scalar')>: <function masked_fill_ at 0x7f2a254bb940>, <OpOverload(op='aten.masked_fill_', overload='Tensor')>: <function masked_fill_ at 0x7f2a254bb940>, <OpOverload(op='aten.mish', overload='default')>: <function mish at 0x7f2a25461040>, <OpOverload(op='aten.mish', overload='out')>: <function mish at 0x7f2a25461040>, <OpOverload(op='aten.mish_', overload='default')>: <function mish at 0x7f2a253f8940>, <OpOverload(op='aten.mse_loss', overload='default')>: <function mse_loss at 0x7f2a256c6820>, <OpOverload(op='aten.mse_loss', overload='out')>: <function mse_loss at 0x7f2a256c6820>, <OpOverload(op='aten.mse_loss_backward', overload='default')>: <function mse_loss_backward at 0x7f2a256c6940>, <OpOverload(op='aten.mse_loss_backward', overload='grad_input')>: <function mse_loss_backward at 0x7f2a256c6940>, <OpOverload(op='aten.multi_margin_loss', overload='default')>: <function multi_margin_loss at 0x7f2a2569fdc0>, <OpOverload(op='aten.multi_margin_loss', overload='out')>: <function multi_margin_loss at 0x7f2a2569fdc0>, <OpOverload(op='aten.multilabel_margin_loss_forward', overload='default')>: <function multilabel_margin_loss_forward at 0x7f2a2569fc10>, <OpOverload(op='aten.multilabel_margin_loss_forward', overload='output')>: <function multilabel_margin_loss_forward at 0x7f2a2569fc10>, <OpOverload(op='aten.mv', overload='default')>: <function mv at 0x7f2a25695ca0>, <OpOverload(op='aten.mv', overload='out')>: <function mv at 0x7f2a25695ca0>, <OpOverload(op='aten.mvlgamma', overload='default')>: <function multigammaln at 0x7f2a25411670>, <OpOverload(op='aten.mvlgamma', overload='out')>: <function multigammaln at 0x7f2a25411670>, <OpOverload(op='aten.mvlgamma_', overload='default')>: <function _make_alias.<locals>._fn at 0x7f2a2547e820>, <OpOverload(op='aten.nansum', overload='default')>: <function nansum at 0x7f2a2569fa60>, <OpOverload(op='aten.nansum', overload='out')>: <function nansum at 0x7f2a2569fa60>, <OpOverload(op='aten.nan_to_num', overload='default')>: <function nan_to_num at 0x7f2a255d90d0>, <OpOverload(op='aten.nan_to_num', overload='out')>: <function nan_to_num at 0x7f2a255d90d0>, <OpOverload(op='aten.nan_to_num_', overload='default')>: <function nan_to_num at 0x7f2a2547e8b0>, <OpOverload(op='aten.native_batch_norm_backward', overload='default')>: <function native_batch_norm_backward at 0x7f2a2566fa60>, <OpOverload(op='aten.native_batch_norm_backward', overload='out')>: <function native_batch_norm_backward at 0x7f2a2566fa60>, <OpOverload(op='aten.native_dropout_backward', overload='default')>: <function native_dropout_backward at 0x7f2a256d1e50>, <OpOverload(op='aten.native_dropout_backward', overload='out')>: <function native_dropout_backward at 0x7f2a256d1e50>, <OpOverload(op='aten.native_group_norm_backward', overload='default')>: <function native_group_norm_backward at 0x7f2a2566f0d0>, <OpOverload(op='aten.native_group_norm_backward', overload='out')>: <function native_group_norm_backward at 0x7f2a2566f0d0>, <OpOverload(op='aten.native_layer_norm_backward', overload='default')>: <function native_layer_norm_backward at 0x7f2a2566f1f0>, <OpOverload(op='aten.native_layer_norm_backward', overload='out')>: <function native_layer_norm_backward at 0x7f2a2566f1f0>, <OpOverload(op='aten.new_empty', overload='default')>: <function new_empty at 0x7f2a254ae5e0>, <OpOverload(op='aten.new_empty', overload='out')>: <function new_empty at 0x7f2a254ae5e0>, <OpOverload(op='aten.new_full', overload='default')>: <function new_full at 0x7f2a254aea60>, <OpOverload(op='aten.new_full', overload='out')>: <function new_full at 0x7f2a254aea60>, <OpOverload(op='aten.new_ones', overload='default')>: <function new_ones at 0x7f2a254ae8b0>, <OpOverload(op='aten.new_ones', overload='out')>: <function new_ones at 0x7f2a254ae8b0>, <OpOverload(op='aten.new_zeros', overload='default')>: <function new_zeros at 0x7f2a254ae700>, <OpOverload(op='aten.new_zeros', overload='out')>: <function new_zeros at 0x7f2a254ae700>, <OpOverload(op='aten.nll_loss_backward', overload='default')>: <function nll_loss_backward at 0x7f2a256d1160>, <OpOverload(op='aten.nll_loss_backward', overload='grad_input')>: <function nll_loss_backward at 0x7f2a256d1160>, <OpOverload(op='aten.nll_loss_forward', overload='default')>: <function nll_loss_forward at 0x7f2a256951f0>, <OpOverload(op='aten.nll_loss_forward', overload='output')>: <function nll_loss_forward at 0x7f2a256951f0>, <OpOverload(op='aten.norm', overload='Scalar')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='ScalarOpt_dim')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='names_ScalarOpt_dim')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='ScalarOpt_dim_dtype')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='dtype_out')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='out')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='ScalarOpt_dtype')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='ScalarOpt_dtype_out')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='Scalar_out')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='names_ScalarOpt_dim_dtype')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='names_dtype_out')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.norm', overload='names_out')>: <function norm at 0x7f2a254bbc10>, <OpOverload(op='aten.ones', overload='default')>: <function ones at 0x7f2a254ae9d0>, <OpOverload(op='aten.ones_like', overload='default')>: <function ones_like at 0x7f2a254bb5e0>, <OpOverload(op='aten.ones_like', overload='out')>: <function ones_like at 0x7f2a254bb5e0>, <OpOverload(op='aten._prelu_kernel', overload='default')>: <function _prelu_kernel at 0x7f2a256c60d0>, <OpOverload(op='aten._prelu_kernel_backward', overload='default')>: <function _prelu_kernel_backward at 0x7f2a256c6160>, <OpOverload(op='aten._reshape_alias', overload='default')>: <function _reshape_alias at 0x7f2a25695040>, <OpOverload(op='aten.rad2deg', overload='default')>: <function rad2deg at 0x7f2a254dd0d0>, <OpOverload(op='aten.rad2deg', overload='out')>: <function rad2deg at 0x7f2a254dd0d0>, <OpOverload(op='aten.rad2deg_', overload='default')>: <function rad2deg at 0x7f2a2547eb80>, <OpOverload(op='aten.renorm', overload='default')>: <function renorm at 0x7f2a2551f310>, <OpOverload(op='aten.renorm', overload='out')>: <function renorm at 0x7f2a2551f310>, <OpOverload(op='aten.renorm_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2562bee0>, <OpOverload(op='aten.rot90', overload='default')>: <function rot90 at 0x7f2a2551f790>, <OpOverload(op='aten.rot90', overload='out')>: <function rot90 at 0x7f2a2551f790>, <OpOverload(op='aten.rrelu_with_noise', overload='default')>: <function rrelu_with_noise at 0x7f2a256c6310>, <OpOverload(op='aten.rrelu_with_noise', overload='out')>: <function rrelu_with_noise at 0x7f2a256c6310>, <OpOverload(op='aten.rrelu_with_noise_', overload='default')>: <function rrelu_with_noise_ at 0x7f2a256c6430>, <OpOverload(op='aten.rsub', overload='Scalar')>: <function rsub_Scalar at 0x7f2a256de790>, <OpOverload(op='aten.rsub', overload='Tensor')>: <function rsub_Tensor at 0x7f2a256de4c0>, <OpOverload(op='aten.select_backward', overload='default')>: <function select_backward at 0x7f2a256d19d0>, <OpOverload(op='aten.select_backward', overload='out')>: <function select_backward at 0x7f2a256d19d0>, <OpOverload(op='aten.sgn', overload='default')>: <function sgn at 0x7f2a255e33a0>, <OpOverload(op='aten.sgn', overload='out')>: <function sgn at 0x7f2a255e33a0>, <OpOverload(op='aten.sgn_', overload='default')>: <function sgn at 0x7f2a2547edc0>, <OpOverload(op='aten.sigmoid_backward', overload='default')>: <function sigmoid_backward at 0x7f2a256b5160>, <OpOverload(op='aten.sigmoid_backward', overload='grad_input')>: <function sigmoid_backward at 0x7f2a256b5160>, <OpOverload(op='aten.silu', overload='default')>: <function silu at 0x7f2a256b5ee0>, <OpOverload(op='aten.silu', overload='out')>: <function silu at 0x7f2a256b5ee0>, <OpOverload(op='aten.silu_', overload='default')>: <function register_inplace.<locals>.inplace_op at 0x7f2a2563d1f0>, <OpOverload(op='aten.silu_backward', overload='default')>: <function silu_backward at 0x7f2a256c6040>, <OpOverload(op='aten.silu_backward', overload='grad_input')>: <function silu_backward at 0x7f2a256c6040>, <OpOverload(op='aten.sinc', overload='default')>: <function sinc at 0x7f2a255e3ee0>, <OpOverload(op='aten.sinc', overload='out')>: <function sinc at 0x7f2a255e3ee0>, <OpOverload(op='aten.sinc_', overload='default')>: <function sinc at 0x7f2a2548b040>, <OpOverload(op='aten.slice_backward', overload='default')>: <function slice_backward at 0x7f2a256d18b0>, <OpOverload(op='aten.slice_backward', overload='out')>: <function slice_backward at 0x7f2a256d18b0>, <OpOverload(op='aten.smooth_l1_loss', overload='default')>: <function smooth_l1_loss at 0x7f2a256c6a60>, <OpOverload(op='aten.smooth_l1_loss', overload='out')>: <function smooth_l1_loss at 0x7f2a256c6a60>, <OpOverload(op='aten.smooth_l1_loss_backward', overload='default')>: <function smooth_l1_loss_backward at 0x7f2a256c6b80>, <OpOverload(op='aten.smooth_l1_loss_backward', overload='grad_input')>: <function smooth_l1_loss_backward_out at 0x7f2a256c6ca0>, <OpOverload(op='aten.soft_margin_loss', overload='default')>: <function soft_margin_loss at 0x7f2a256d1670>, <OpOverload(op='aten.soft_margin_loss', overload='out')>: <function soft_margin_loss at 0x7f2a256d1670>, <OpOverload(op='aten.soft_margin_loss_backward', overload='default')>: <function soft_margin_loss_backward at 0x7f2a256d1700>, <OpOverload(op='aten.soft_margin_loss_backward', overload='grad_input')>: <function soft_margin_loss_backward at 0x7f2a256d1700>, <OpOverload(op='aten._softmax_backward_data', overload='default')>: <function _softmax_backward_data at 0x7f2a256d1c10>, <OpOverload(op='aten._softmax_backward_data', overload='out')>: <function _softmax_backward_data at 0x7f2a256d1c10>, <OpOverload(op='aten.softplus', overload='default')>: <function softplus at 0x7f2a254615e0>, <OpOverload(op='aten.softplus', overload='out')>: <function softplus at 0x7f2a254615e0>, <OpOverload(op='aten.softplus_backward', overload='default')>: <function softplus_backward at 0x7f2a256b5280>, <OpOverload(op='aten.softplus_backward', overload='grad_input')>: <function softplus_backward at 0x7f2a256b5280>, <OpOverload(op='aten.softshrink', overload='default')>: <function softshrink at 0x7f2a25461af0>, <OpOverload(op='aten.softshrink', overload='out')>: <function softshrink at 0x7f2a25461af0>, <OpOverload(op='aten.special_entr', overload='default')>: <function entr at 0x7f2a254054c0>, <OpOverload(op='aten.special_entr', overload='out')>: <function entr at 0x7f2a254054c0>, <OpOverload(op='aten.special_log_ndtr', overload='default')>: <function log_ndtr at 0x7f2a25411160>, <OpOverload(op='aten.special_log_ndtr', overload='out')>: <function log_ndtr at 0x7f2a25411160>, <OpOverload(op='aten.special_xlog1py', overload='default')>: <function xlog1py at 0x7f2a254114c0>, <OpOverload(op='aten.special_xlog1py', overload='other_scalar')>: <function xlog1py at 0x7f2a254114c0>, <OpOverload(op='aten.special_xlog1py', overload='self_scalar')>: <function xlog1py at 0x7f2a254114c0>, <OpOverload(op='aten.special_xlog1py', overload='out')>: <function xlog1py at 0x7f2a254114c0>, <OpOverload(op='aten.special_xlog1py', overload='self_scalar_out')>: <function xlog1py at 0x7f2a254114c0>, <OpOverload(op='aten.special_xlog1py', overload='other_scalar_out')>: <function xlog1py at 0x7f2a254114c0>, <OpOverload(op='aten.stack', overload='default')>: <function stack at 0x7f2a2551f9d0>, <OpOverload(op='aten.stack', overload='out')>: <function stack at 0x7f2a2551f9d0>, <OpOverload(op='aten.t', overload='default')>: <function t at 0x7f2a2551f820>, <OpOverload(op='aten.tanh_backward', overload='default')>: <function tanh_backward at 0x7f2a256b5040>, <OpOverload(op='aten.tanh_backward', overload='grad_input')>: <function tanh_backward at 0x7f2a256b5040>, <OpOverload(op='aten.threshold', overload='default')>: <function threshold at 0x7f2a253ed790>, <OpOverload(op='aten.threshold', overload='out')>: <function threshold at 0x7f2a253ed790>, <OpOverload(op='aten.threshold_', overload='default')>: <function threshold at 0x7f2a253f8a60>, <OpOverload(op='aten.threshold_backward', overload='default')>: <function threshold_backward at 0x7f2a256b5a60>, <OpOverload(op='aten.threshold_backward', overload='grad_input')>: <function threshold_backward at 0x7f2a256b5a60>, <OpOverload(op='aten.trace', overload='default')>: <function trace at 0x7f2a254bbaf0>, <OpOverload(op='aten.trace', overload='out')>: <function trace at 0x7f2a254bbaf0>, <OpOverload(op='aten.transpose', overload='int')>: <function transpose at 0x7f2a2551fd30>, <OpOverload(op='aten.tril', overload='default')>: <function tril at 0x7f2a254cb160>, <OpOverload(op='aten.tril', overload='out')>: <function tril at 0x7f2a254cb160>, <OpOverload(op='aten.tril_', overload='default')>: <function tril at 0x7f2a2548b430>, <OpOverload(op='aten.triu', overload='default')>: <function triu at 0x7f2a254cb040>, <OpOverload(op='aten.triu', overload='out')>: <function triu at 0x7f2a254cb040>, <OpOverload(op='aten.triu_', overload='default')>: <function triu at 0x7f2a2548b4c0>, <OpOverload(op='aten.unfold_backward', overload='default')>: <function unfold_backward at 0x7f2a256de280>, <OpOverload(op='aten.unfold_backward', overload='out')>: <function unfold_backward at 0x7f2a256de280>, <OpOverload(op='aten.unfold_copy', overload='default')>: <function unfold_copy at 0x7f2a2551ff70>, <OpOverload(op='aten.unfold_copy', overload='out')>: <function unfold_copy at 0x7f2a2551ff70>, <OpOverload(op='aten._unsafe_index', overload='Tensor')>: <function _index at 0x7f2a256950d0>, <OpOverload(op='aten.upsample_bilinear2d', overload='vec')>: <function upsample_bilinear2d_vec at 0x7f2a2568eee0>, <OpOverload(op='aten.upsample_bilinear2d', overload='default')>: <function upsample_bilinear2d at 0x7f2a2568ef70>, <OpOverload(op='aten.xlogy', overload='Tensor')>: <function xlogy at 0x7f2a25556dc0>, <OpOverload(op='aten.xlogy', overload='Scalar_Other')>: <function xlogy at 0x7f2a25556dc0>, <OpOverload(op='aten.xlogy', overload='Scalar_Self')>: <function xlogy at 0x7f2a25556dc0>, <OpOverload(op='aten.xlogy', overload='OutTensor')>: <function xlogy at 0x7f2a25556dc0>, <OpOverload(op='aten.xlogy', overload='OutScalar_Self')>: <function xlogy at 0x7f2a25556dc0>, <OpOverload(op='aten.xlogy', overload='OutScalar_Other')>: <function xlogy at 0x7f2a25556dc0>, <OpOverload(op='aten.xlogy_', overload='Tensor')>: <function xlogy at 0x7f2a2548b670>, <OpOverload(op='aten.xlogy_', overload='Scalar_Other')>: <function xlogy at 0x7f2a2548b670>, <OpOverload(op='aten.zero', overload='default')>: <function zero at 0x7f2a255b53a0>, <OpOverload(op='aten.zero', overload='out')>: <function zero at 0x7f2a255b53a0>, <OpOverload(op='aten.zero_', overload='default')>: <function zero at 0x7f2a2548b9d0>, <OpOverload(op='aten.zeros', overload='default')>: <function zeros at 0x7f2a254ae820>, <OpOverload(op='aten.zeros_like', overload='default')>: <function zeros_like at 0x7f2a254bb550>, <OpOverload(op='aten.zeros_like', overload='out')>: <function zeros_like at 0x7f2a254bb550>, <OpOverload(op='aten.arange', overload='out')>: <function arange_default at 0x7f2a2569fb80>, <OpOverload(op='aten.arange', overload='start_step')>: <function arange at 0x7f2a254aeca0>, <OpOverload(op='aten.arange', overload='start_out')>: <function arange at 0x7f2a254aeca0>, <OpOverload(op='aten.bitwise_and_', overload='Tensor')>: <function bitwise_and at 0x7f2a254dd9d0>, <OpOverload(op='aten.bitwise_and_', overload='Scalar')>: <function bitwise_and at 0x7f2a254dd9d0>, <OpOverload(op='aten.bitwise_or_', overload='Tensor')>: <function bitwise_or at 0x7f2a254ddb80>, <OpOverload(op='aten.bitwise_or_', overload='Scalar')>: <function bitwise_or at 0x7f2a254ddb80>, <OpOverload(op='aten.clamp_min_', overload='default')>: <function clamp_min at 0x7f2a254dde50>, <OpOverload(op='aten.clamp_min_', overload='Tensor')>: <function clamp_min at 0x7f2a254dde50>, <OpOverload(op='aten.dist', overload='default')>: <function dist at 0x7f2a256d1790>, <OpOverload(op='aten.dist', overload='out')>: <function dist at 0x7f2a256d1790>, <OpOverload(op='aten.flip', overload='default')>: <function flip at 0x7f2a25517b80>, <OpOverload(op='aten.flip', overload='out')>: <function flip at 0x7f2a25517b80>, <OpOverload(op='aten.gelu', overload='default')>: <function gelu at 0x7f2a253f8040>, <OpOverload(op='aten.gelu', overload='out')>: <function gelu at 0x7f2a253f8040>, <OpOverload(op='aten.hardtanh', overload='default')>: <function hardtanh at 0x7f2a253edaf0>, <OpOverload(op='aten.hardtanh', overload='out')>: <function hardtanh at 0x7f2a253edaf0>, <OpOverload(op='aten.index_select', overload='default')>: <function index_select at 0x7f2a259bf550>, <OpOverload(op='aten.index_select', overload='out')>: <function index_select at 0x7f2a259bf550>, <OpOverload(op='aten.index_select', overload='dimname')>: <function index_select at 0x7f2a259bf550>, <OpOverload(op='aten.index_select', overload='dimname_out')>: <function index_select at 0x7f2a259bf550>, <OpOverload(op='aten.lcm', overload='default')>: <function lcm at 0x7f2a2553a700>, <OpOverload(op='aten.lcm', overload='out')>: <function lcm at 0x7f2a2553a700>, <OpOverload(op='aten.leaky_relu', overload='default')>: <function leaky_relu at 0x7f2a25456dc0>, <OpOverload(op='aten.leaky_relu', overload='out')>: <function leaky_relu at 0x7f2a25456dc0>, <OpOverload(op='aten.linalg_vector_norm', overload='default')>: <function vector_norm at 0x7f2a254473a0>, <OpOverload(op='aten.linalg_vector_norm', overload='out')>: <function vector_norm at 0x7f2a254473a0>, <OpOverload(op='aten._log_softmax', overload='default')>: <function _log_softmax at 0x7f2a256de700>, <OpOverload(op='aten._log_softmax', overload='out')>: <function _log_softmax at 0x7f2a256de700>, <OpOverload(op='aten._native_batch_norm_legit', overload='default')>: <function _native_batch_norm_legit at 0x7f2a2566f550>, <OpOverload(op='aten._native_batch_norm_legit', overload='no_stats')>: <function _native_batch_norm_legit_no_stats at 0x7f2a2566f5e0>, <OpOverload(op='aten._native_batch_norm_legit_functional', overload='default')>: <function _native_batch_norm_legit_functional at 0x7f2a2566f670>, <OpOverload(op='aten._native_batch_norm_legit_no_training', overload='default')>: <function _native_batch_norm_legit_no_training at 0x7f2a2566f4c0>, <OpOverload(op='aten.native_batch_norm', overload='default')>: <function native_batch_norm at 0x7f2a2566f310>, <OpOverload(op='aten.native_batch_norm', overload='out')>: <function native_batch_norm at 0x7f2a2566f310>, <OpOverload(op='aten.native_group_norm', overload='default')>: <function native_group_norm at 0x7f2a2551f040>, <OpOverload(op='aten.native_layer_norm', overload='default')>: <function native_layer_norm at 0x7f2a2551f0d0>, <OpOverload(op='aten.native_layer_norm', overload='out')>: <function native_layer_norm at 0x7f2a2551f0d0>, <OpOverload(op='aten._softmax', overload='default')>: <function _softmax at 0x7f2a256de5e0>, <OpOverload(op='aten._softmax', overload='out')>: <function _softmax at 0x7f2a256de5e0>, <OpOverload(op='aten.sin_', overload='default')>: <function sin at 0x7f2a2547ef70>, <OpOverload(op='aten.sqrt_', overload='default')>: <function sqrt at 0x7f2a2548b160>, <OpOverload(op='aten.std', overload='default')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='dim')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='correction')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='names_dim')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='names_out')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='out')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='correction_out')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='correction_names')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std', overload='correction_names_out')>: <function std at 0x7f2a254feaf0>, <OpOverload(op='aten.std_mean', overload='correction')>: <function std_mean at 0x7f2a254feb80>, <OpOverload(op='aten._to_copy', overload='default')>: <function _to_copy at 0x7f2a2566f820>, <OpOverload(op='aten._to_copy', overload='out')>: <function _to_copy at 0x7f2a2566f820>, <OpOverload(op='aten.tril_indices', overload='default')>: <function tril_indices at 0x7f2a254cb280>, <OpOverload(op='aten.tril_indices', overload='out')>: <function tril_indices at 0x7f2a254cb280>, <OpOverload(op='aten.triu_indices', overload='default')>: <function triu_indices at 0x7f2a254cb3a0>, <OpOverload(op='aten.triu_indices', overload='out')>: <function triu_indices at 0x7f2a254cb3a0>, <OpOverload(op='aten.unsafe_split', overload='Tensor')>: <function split at 0x7f2a256dea60>, <OpOverload(op='aten._unsafe_view', overload='default')>: <function _unsafe_view at 0x7f23103c4790>, <OpOverload(op='aten._assert_async', overload='msg')>: <function assert_async_msg_decomp at 0x7f23103c4ca0>, <OpOverload(op='aten._functional_assert_async', overload='msg')>: <function functional_assert_async_msg_decomp at 0x7f23103c4940>, <OpOverload(op='aten.sym_constrain_range_for_size', overload='default')>: <function sym_constrain_range_for_size at 0x7f23103c48b0>, <OpOverload(op='aten.clamp', overload='default')>: <function clamp at 0x7f23103c4a60>, <OpOverload(op='aten.clamp', overload='Tensor')>: <function clamp at 0x7f23103c4a60>, <OpOverload(op='aten.clamp', overload='out')>: <function clamp at 0x7f23103c4a60>, <OpOverload(op='aten.clamp', overload='Tensor_out')>: <function clamp at 0x7f23103c4a60>, <OpOverload(op='aten.floor_divide', overload='default')>: <function floordiv at 0x7f23103c4820>, <OpOverload(op='aten.empty_permuted', overload='default')>: <function empty_permuted at 0x7f23103c40d0>, <OpOverload(op='aten.convolution_backward', overload='default')>: <function convolution_backward at 0x7f23103c41f0>, <OpOverload(op='aten.convolution_backward', overload='out')>: <function convolution_backward at 0x7f23103c41f0>, <OpOverload(op='aten.log2', overload='default')>: <function log2 at 0x7f23103c4d30>, <OpOverload(op='aten.log2', overload='out')>: <function log2 at 0x7f23103c4d30>, <OpOverload(op='aten.round', overload='decimals')>: <function round_dec at 0x7f23103c4b80>, <OpOverload(op='aten.all', overload='default')>: <function all at 0x7f23103c4ee0>, <OpOverload(op='aten.all', overload='dim')>: <function all_dim at 0x7f23103c4c10>, <OpOverload(op='aten.baddbmm', overload='default')>: <function baddbmm at 0x7f23103c4e50>, <OpOverload(op='aten.baddbmm', overload='out')>: <function baddbmm at 0x7f23103c4e50>, <OpOverload(op='aten.bmm', overload='default')>: <function bmm at 0x7f23103c4f70>, <OpOverload(op='aten.bmm', overload='out')>: <function bmm at 0x7f23103c4f70>, <OpOverload(op='aten.mm', overload='default')>: <function mm at 0x7f23103f1040>, <OpOverload(op='aten.mm', overload='out')>: <function mm at 0x7f23103f1040>, <OpOverload(op='aten.cat', overload='default')>: <function cat at 0x7f23103f10d0>, <OpOverload(op='aten.angle', overload='default')>: <function angle at 0x7f23103f1160>, <OpOverload(op='aten.angle', overload='out')>: <function angle at 0x7f23103f1160>, <OpOverload(op='aten.conj_physical', overload='default')>: <function conj_physical at 0x7f23103f11f0>, <OpOverload(op='aten.conj_physical', overload='out')>: <function conj_physical at 0x7f23103f11f0>, <OpOverload(op='aten.lift', overload='default')>: <function lift at 0x7f23103f1280>, <OpOverload(op='aten.lift', overload='out')>: <function lift at 0x7f23103f1280>, <OpOverload(op='aten.detach_', overload='default')>: <function lift at 0x7f23103f1280>, <OpOverload(op='aten.bernoulli', overload='default')>: <function bernoulli at 0x7f23103f1310>, <OpOverload(op='aten.fmin', overload='default')>: <function fmin at 0x7f23103f13a0>, <OpOverload(op='aten.fmin', overload='out')>: <function fmin at 0x7f23103f13a0>, <OpOverload(op='prims.fmin', overload='default')>: <function fmin at 0x7f23103f13a0>, <OpOverload(op='aten.fmax', overload='default')>: <function fmax at 0x7f23103f1430>, <OpOverload(op='aten.fmax', overload='out')>: <function fmax at 0x7f23103f1430>, <OpOverload(op='prims.fmax', overload='default')>: <function fmax at 0x7f23103f1430>, <OpOverload(op='aten.narrow_copy', overload='default')>: <function narrow_copy at 0x7f23103f14c0>, <OpOverload(op='aten.narrow_copy', overload='out')>: <function narrow_copy at 0x7f23103f14c0>, <OpOverload(op='aten.expand_copy', overload='default')>: <function expand_copy at 0x7f23103f1550>, <OpOverload(op='aten.expand_copy', overload='out')>: <function expand_copy at 0x7f23103f1550>, <OpOverload(op='aten.view_copy', overload='default')>: <function view_copy_default at 0x7f23103f15e0>, <OpOverload(op='aten.view_copy', overload='dtype')>: <function view_copy_dtype at 0x7f23103f1670>, <OpOverload(op='aten.rand_like', overload='default')>: <function rand_like at 0x7f23103f1700>, <OpOverload(op='aten.rand_like', overload='out')>: <function rand_like at 0x7f23103f1700>, <OpOverload(op='aten.randn_like', overload='default')>: <function randn_like at 0x7f23103f1790>, <OpOverload(op='aten.randn_like', overload='out')>: <function randn_like at 0x7f23103f1790>, <OpOverload(op='aten.full_like', overload='default')>: <function full_like at 0x7f23103f1820>, <OpOverload(op='aten.full_like', overload='out')>: <function full_like at 0x7f23103f1820>, <OpOverload(op='aten.randint_like', overload='default')>: <function randint_like at 0x7f23103f18b0>, <OpOverload(op='aten.randint_like', overload='low_dtype')>: <function randint_like_low at 0x7f23103f1940>, <OpOverload(op='aten.randint', overload='default')>: <function randint at 0x7f23103f19d0>, <OpOverload(op='quantized_decomposed.quantize_per_tensor', overload='default')>: <function quantize_per_tensor_default_decomp_impl at 0x7f23103f1a60>, <OpOverload(op='quantized_decomposed.dequantize_per_tensor', overload='default')>: <function dequantize_per_tensor_default_decomp_impl at 0x7f23103f1af0>, <OpOverload(op='quantized_decomposed.quantize_per_tensor', overload='tensor')>: <function quantize_per_tensor_tensor_decomp_impl at 0x7f23103f1b80>, <OpOverload(op='quantized_decomposed.dequantize_per_tensor', overload='tensor')>: <function dequantize_per_tensor_tensor_decomp_impl at 0x7f23103f1c10>, <OpOverload(op='aten._foreach_addcmul', overload='Scalar')>: <function _foreach_addcmul_scalar at 0x7f23103f1ca0>, <OpOverload(op='aten._foreach_addcdiv', overload='Scalar')>: <function _foreach_addcdiv_scalar at 0x7f23103f1d30>, <OpOverload(op='aten._foreach_lerp', overload='Scalar')>: <function _foreach_lerp_scalar at 0x7f23103f1dc0>, <OpOverload(op='aten.miopen_batch_norm', overload='default')>: <function miopen_batch_norm at 0x7f23103f1ee0>, <OpOverload(op='aten.miopen_batch_norm', overload='out')>: <function miopen_batch_norm at 0x7f23103f1ee0>}\n",
      "{<OpOverload(op='aten.add', overload='Tensor')>: <function add at 0x7f2a25579280>, <OpOverload(op='aten.add', overload='Scalar')>: <function add at 0x7f2a25579280>, <OpOverload(op='aten.add', overload='out')>: <function add at 0x7f2a25579280>, <OpOverload(op='aten.add', overload='Scalar_out')>: <function add at 0x7f2a25579280>, <OpOverload(op='aten.expand', overload='default')>: <function expand at 0x7f2a25517940>}\n"
     ]
    }
   ],
   "source": [
    "# PrimTorch example\n",
    "from torch._inductor.decomposition import decompositions as default_decompositions\n",
    "print(default_decompositions)\n",
    "\n",
    "def fn(a, b):\n",
    "    return a + b\n",
    "\n",
    "prims_decomp = torch._inductor.decomposition.get_decompositions([\n",
    "    torch.ops.aten.add,\n",
    "    torch.ops.aten.expand.default,\n",
    "])\n",
    "\n",
    "print(prims_decomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d241b2",
   "metadata": {},
   "source": [
    "### New logging  api\n",
    "print(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a287daf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-micro-benchmarking'...\n",
      "remote: Enumerating objects: 141, done.\u001b[K\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 141 (delta 9), reused 9 (delta 4), pack-reused 117\u001b[K\n",
      "Receiving objects: 100% (141/141), 52.72 KiB | 199.00 KiB/s, done.\n",
      "Resolving deltas: 100% (74/74), done.\n"
     ]
    }
   ],
   "source": [
    "# Updated logging api\n",
    "import torch._logging \n",
    "torch._logging.set_logs(dynamo = logging.DEBUG,\n",
    "                        dynamic = logging.DEBUG,\n",
    "                        output_code=True,\n",
    "                        guards=True,\n",
    "                        bytecode=True,\n",
    "                        graph=True,\n",
    "                        graph_code=True,\n",
    "                        aot_graphs=True,\n",
    "                        aot_joint_graphs=True,\n",
    "                        aot=logging.DEBUG,\n",
    "                        graph_sizes=True,\n",
    "                        aot_joint_graph=True,\n",
    "                        recompiles=True,\n",
    "                        graph_breaks=True,\n",
    "                        not_implemented=True,\n",
    "                        schedule=True,\n",
    "                        perf_hints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec372313",
   "metadata": {},
   "source": [
    "### Inductor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da445f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-08-18 08:59:44 223465:223465 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "[2023-08-18 08:59:44,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /tmp/ipykernel_223465/2649045773.py:27\n",
      "[2023-08-18 08:59:44,888] torch.fx.experimental.symbolic_shapes: [INFO] 2.0: create_env\n",
      "[2023-08-18 08:59:44,891] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_223465/2649045773.py:27\n",
      "[2023-08-18 08:59:44,891] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         def forward(self, x):\n",
      "[2023-08-18 08:59:44,893] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['x'] (8, 32) [<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>] [None, None]\n",
      "[2023-08-18 08:59:44,896] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_223465/2649045773.py:28\n",
      "[2023-08-18 08:59:44,896] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             x = self.fc1(x)\n",
      "[2023-08-18 08:59:44,896] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-08-18 08:59:44,897] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR fc1 [NNModuleVariable()]\n",
      "[2023-08-18 08:59:44,898] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [NNModuleVariable()]\n",
      "[2023-08-18 08:59:44,899] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [NNModuleVariable(), TensorVariable()]\n",
      "[2023-08-18 08:59:44,914] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST x [TensorVariable()]\n",
      "[2023-08-18 08:59:44,917] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_223465/2649045773.py:29\n",
      "[2023-08-18 08:59:44,917] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             x = torch.nn.functional.gelu(x)\n",
      "[2023-08-18 08:59:44,918] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-08-18 08:59:44,919] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR nn [TorchVariable(<module 'torch' from '/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/__init__.py'>)]\n",
      "[2023-08-18 08:59:44,920] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR functional [TorchVariable(<module 'torch.nn' from '/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/__init__.py'>)]\n",
      "[2023-08-18 08:59:44,922] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR gelu [TorchVariable(<module 'torch.nn.functional' from '/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/functional.py'>)]\n",
      "[2023-08-18 08:59:44,923] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [TorchVariable(<built-in function gelu>)]\n",
      "[2023-08-18 08:59:44,924] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [TorchVariable(<built-in function gelu>), TensorVariable()]\n",
      "[2023-08-18 08:59:44,929] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST x [TensorVariable()]\n",
      "[2023-08-18 08:59:44,930] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_223465/2649045773.py:30\n",
      "[2023-08-18 08:59:44,930] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             return x\n",
      "[2023-08-18 08:59:44,930] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []\n",
      "[2023-08-18 08:59:44,931] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-08-18 08:59:44,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-08-18 08:59:44,932] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-08-18 08:59:44,932] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_223465/2649045773.py, line 30 in forward>], graph_break=False)\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]  ===== __compiled_fn_2 =====\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]  <eval_with_key>.131 class GraphModule(torch.nn.Module):\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]     def forward(self, L_x_ : torch.Tensor):\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_x_ = L_x_\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_223465/2649045773.py:28, code: x = self.fc1(x)\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         l__self___fc1 = self.L__self___fc1(l_x_);  l_x_ = None\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_223465/2649045773.py:29, code: x = torch.nn.functional.gelu(x)\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         gelu = torch._C._nn.gelu(l__self___fc1);  l__self___fc1 = None\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         return (gelu,)\n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-08-18 08:59:44,934] torch._dynamo.output_graph.__graph_code: [DEBUG] \n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG]  __compiled_fn_2 <eval_with_key>.131 opcode         name           target                    args              kwargs\n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG] -------------  -------------  ------------------------  ----------------  --------\n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG] placeholder    l_x_           L_x_                      ()                {}\n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG] call_module    l__self___fc1  L__self___fc1             (l_x_,)           {}\n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG] call_function  gelu           <built-in function gelu>  (l__self___fc1,)  {}\n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG] output         output         output                    ((gelu,),)        {}\n",
      "[2023-08-18 08:59:44,935] torch._dynamo.output_graph.__graph: [DEBUG] \n",
      "[2023-08-18 08:59:44,936] torch._dynamo.output_graph.__graph_sizes: [DEBUG] TRACED GRAPH TENSOR SIZES\n",
      "[2023-08-18 08:59:44,936] torch._dynamo.output_graph.__graph_sizes: [DEBUG] ===== __compiled_fn_2 =====\n",
      "[2023-08-18 08:59:44,936] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l_x_: (8, 32)\n",
      "[2023-08-18 08:59:44,936] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l__self___fc1: (8, 64)\n",
      "[2023-08-18 08:59:44,936] torch._dynamo.output_graph.__graph_sizes: [DEBUG] gelu: (8, 64)\n",
      "[2023-08-18 08:59:44,936] torch._dynamo.output_graph.__graph_sizes: [DEBUG] \n",
      "[2023-08-18 08:59:44,937] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO] TRACED GRAPH\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]  ===== Joint graph 15 =====\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]  <eval_with_key>.135 from /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:477 in wrapped class joint_helper(torch.nn.Module):\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]     def forward(self, primals, tangents):\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[8, 32], tangents_1: f32[8, 64], = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         # File: /tmp/ipykernel_223465/2649045773.py:28, code: x = self.fc1(x)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         permute: f32[32, 64] = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         addmm: f32[8, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, permute);  primals_2 = permute = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         \n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         # File: /tmp/ipykernel_223465/2649045773.py:29, code: x = torch.nn.functional.gelu(x)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, 0.5)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_1: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, 0.7071067811865476)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         erf: f32[8, 64] = torch.ops.aten.erf.default(mul_1);  mul_1 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         add: f32[8, 64] = torch.ops.aten.add.Tensor(erf, 1);  erf = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_2: f32[8, 64] = torch.ops.aten.mul.Tensor(mul, add);  mul = add = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_3: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, 0.7071067811865476)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         erf_1: f32[8, 64] = torch.ops.aten.erf.default(mul_3);  mul_3 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         add_1: f32[8, 64] = torch.ops.aten.add.Tensor(erf_1, 1);  erf_1 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_4: f32[8, 64] = torch.ops.aten.mul.Tensor(add_1, 0.5);  add_1 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_5: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, addmm)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_6: f32[8, 64] = torch.ops.aten.mul.Tensor(mul_5, -0.5);  mul_5 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         exp: f32[8, 64] = torch.ops.aten.exp.default(mul_6);  mul_6 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_7: f32[8, 64] = torch.ops.aten.mul.Tensor(exp, 0.3989422804014327);  exp = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_8: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, mul_7);  addmm = mul_7 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         add_2: f32[8, 64] = torch.ops.aten.add.Tensor(mul_4, mul_8);  mul_4 = mul_8 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mul_9: f32[8, 64] = torch.ops.aten.mul.Tensor(tangents_1, add_2);  tangents_1 = add_2 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         \n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         # File: /tmp/ipykernel_223465/2649045773.py:28, code: x = self.fc1(x)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         permute_1: f32[64, 8] = torch.ops.aten.permute.default(mul_9, [1, 0])\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         mm: f32[64, 32] = torch.ops.aten.mm.default(permute_1, primals_3);  permute_1 = primals_3 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         permute_2: f32[32, 64] = torch.ops.aten.permute.default(mm, [1, 0]);  mm = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         sum_1: f32[1, 64] = torch.ops.aten.sum.dim_IntList(mul_9, [0], True);  mul_9 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         view: f32[64] = torch.ops.aten.view.default(sum_1, [64]);  sum_1 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         permute_3: f32[64, 32] = torch.ops.aten.permute.default(permute_2, [1, 0]);  permute_2 = None\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         return pytree.tree_unflatten([mul_2, permute_3, view, None], self._out_spec)\n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO]         \n",
      "[2023-08-18 08:59:45,010] torch._functorch.aot_autograd.__aot_joint_graph: [INFO] \n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO] TRACED GRAPH\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]  ===== Forward graph 15 =====\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]  <eval_with_key>.141 class GraphModule(torch.nn.Module):\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]     def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[8, 32]):\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_223465/2649045773.py:28, code: x = self.fc1(x)\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute: f32[32, 64] = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         addmm: f32[8, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, permute);  primals_2 = permute = None\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_223465/2649045773.py:29, code: x = torch.nn.functional.gelu(x)\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, 0.5)\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_1: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, 0.7071067811865476)\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         erf: f32[8, 64] = torch.ops.aten.erf.default(mul_1);  mul_1 = None\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         add: f32[8, 64] = torch.ops.aten.add.Tensor(erf, 1);  erf = None\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_2: f32[8, 64] = torch.ops.aten.mul.Tensor(mul, add);  mul = add = None\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         return [mul_2, primals_3, addmm]\n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2023-08-18 08:59:45,049] torch._functorch.aot_autograd.__aot_graphs: [INFO] \n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO] TRACED GRAPH\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]  ===== Backward graph 15 =====\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]  <eval_with_key>.142 class GraphModule(torch.nn.Module):\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]     def forward(self, primals_3: f32[8, 32], addmm: f32[8, 64], tangents_1: f32[8, 64]):\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_223465/2649045773.py:29, code: x = torch.nn.functional.gelu(x)\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_1: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, 0.7071067811865476)\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         erf: f32[8, 64] = torch.ops.aten.erf.default(mul_1);  mul_1 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         add: f32[8, 64] = torch.ops.aten.add.Tensor(erf, 1);  erf = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_4: f32[8, 64] = torch.ops.aten.mul.Tensor(add, 0.5);  add = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_5: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, addmm)\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_6: f32[8, 64] = torch.ops.aten.mul.Tensor(mul_5, -0.5);  mul_5 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         exp: f32[8, 64] = torch.ops.aten.exp.default(mul_6);  mul_6 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_7: f32[8, 64] = torch.ops.aten.mul.Tensor(exp, 0.3989422804014327);  exp = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_8: f32[8, 64] = torch.ops.aten.mul.Tensor(addmm, mul_7);  addmm = mul_7 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         add_2: f32[8, 64] = torch.ops.aten.add.Tensor(mul_4, mul_8);  mul_4 = mul_8 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul_9: f32[8, 64] = torch.ops.aten.mul.Tensor(tangents_1, add_2);  tangents_1 = add_2 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_223465/2649045773.py:28, code: x = self.fc1(x)\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_1: f32[64, 8] = torch.ops.aten.permute.default(mul_9, [1, 0])\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mm: f32[64, 32] = torch.ops.aten.mm.default(permute_1, primals_3);  permute_1 = primals_3 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_2: f32[32, 64] = torch.ops.aten.permute.default(mm, [1, 0]);  mm = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         sum_1: f32[1, 64] = torch.ops.aten.sum.dim_IntList(mul_9, [0], True);  mul_9 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         view: f32[64] = torch.ops.aten.view.default(sum_1, [64]);  sum_1 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_3: f32[64, 32] = torch.ops.aten.permute.default(permute_2, [1, 0]);  permute_2 = None\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         return [permute_3, view, None]\n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2023-08-18 08:59:45,051] torch._functorch.aot_autograd.__aot_graphs: [INFO] \n",
      "[2023-08-18 08:59:45,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-18 08:59:45,084] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: eval True [trivial]\n",
      "[2023-08-18 08:59:45,095] torch._inductor.graph: [DEBUG] Force channels last inputs for 0 conv for the current graph with id 2\n",
      "[2023-08-18 08:59:45,105] torch._inductor.scheduler: [INFO] Number of scheduler nodes after fusion 2\n",
      "[2023-08-18 08:59:45,106] torch._inductor.codegen.triton.__schedule: [DEBUG] Schedule:\n",
      "[2023-08-18 08:59:45,106] torch._inductor.codegen.triton.__schedule: [DEBUG]  [SchedulerNode(name='buf1')]\n",
      "[2023-08-18 08:59:45,116] torch._inductor.graph.__output_code: [INFO] Output code written to: /tmp/torchinductor_jenkins/d6/cd6j3be5un2dpthxmoeoe2i425odk3oxaryh753rzoxyzvzvxdj2.py\n",
      "[2023-08-18 08:59:45,117] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_jenkins/d6/cd6j3be5un2dpthxmoeoe2i425odk3oxaryh753rzoxyzvzvxdj2.py\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] Output code: \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from ctypes import c_void_p, c_long\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import torch\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import math\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import random\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import os\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import tempfile\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from math import inf, nan\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.hooks import run_intermediate_hooks\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.utils import maybe_profile\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch import empty_strided, as_strided, device\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.codecache import AsyncCompile\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.select_algorithm import extern_kernels\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] aten = torch.ops.aten\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] async_compile = AsyncCompile()\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] # kernel path: /tmp/torchinductor_jenkins/4q/c4qymbcfcanwczzakatsyqwrdgxs6gzul4igq3kymzm23a46rm4z.py\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] # Source Nodes: [gelu], Original ATen: [aten.gelu]\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] # gelu => add, erf, mul, mul_1, mul_2\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] triton_poi_fused_gelu_0 = async_compile.triton('triton_', '''\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import triton\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import triton.language as tl\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.ir import ReductionHint\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.ir import TileHint\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.triton_heuristics import AutotuneHint, pointwise\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.utils import instance_descriptor\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor import triton_helpers\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] @pointwise(size_hints=[512], filename=__file__, meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'mutated_arg_names': [], 'autotune_hints': set(), 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] @triton.jit\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     xnumel = 512\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     xoffset = tl.program_id(0) * XBLOCK\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     xmask = xindex < xnumel\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     x0 = xindex\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp0 = tl.load(in_ptr0 + (x0), xmask)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp1 = 0.5\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp2 = tmp0 * tmp1\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp3 = 0.7071067811865476\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp4 = tmp0 * tmp3\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp5 = tl.math.erf(tmp4)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp6 = 1.0\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp7 = tmp5 + tmp6\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tmp8 = tmp2 * tmp7\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     tl.store(out_ptr0 + (x0), tmp8, xmask)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] ''')\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import triton\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] import triton.language as tl\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.triton_heuristics import grid, start_graph, end_graph\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] from torch._C import _cuda_getCurrentRawStream as get_cuda_stream\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] async_compile.wait(globals())\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] del async_compile\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] def call(args):\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     primals_1, primals_2, primals_3 = args\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     args.clear()\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     assert_size_stride(primals_1, (64, 32), (32, 1))\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     assert_size_stride(primals_2, (64, ), (1, ))\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     assert_size_stride(primals_3, (8, 32), (32, 1))\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     with torch.cuda._DeviceGuard(0):\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         torch.cuda.set_device(0) # no-op to ensure context\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         buf0 = empty_strided((8, 64), (64, 1), device='cuda', dtype=torch.float32)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         # Source Nodes: [l__self___fc1], Original ATen: [aten.addmm]\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         extern_kernels.addmm(primals_2, primals_3, as_strided(primals_1, (32, 64), (1, 32)), alpha=1, beta=1, out=buf0)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         del primals_1\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         del primals_2\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         buf1 = empty_strided((8, 64), (64, 1), device='cuda', dtype=torch.float32)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         # Source Nodes: [gelu], Original ATen: [aten.gelu]\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         stream0 = get_cuda_stream(0)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         triton_poi_fused_gelu_0.run(buf0, buf1, 512, grid=grid(512), stream=stream0)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]         return (buf1, primals_3, buf0, )\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] def benchmark_compiled_module(times=10, repeat=10):\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     from torch._dynamo.testing import rand_strided\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     from torch._inductor.utils import print_performance\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     primals_1 = rand_strided((64, 32), (32, 1), device='cuda:0', dtype=torch.float32)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     primals_2 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     primals_3 = rand_strided((8, 32), (32, 1), device='cuda:0', dtype=torch.float32)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     return print_performance(lambda: call([primals_1, primals_2, primals_3]), times=times, repeat=repeat)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] if __name__ == \"__main__\":\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG]     compiled_module_main('None', benchmark_compiled_module)\n",
      "[2023-08-18 08:59:45,118] torch._inductor.graph.__output_code: [DEBUG] \n",
      "[2023-08-18 08:59:45,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2\n",
      "[2023-08-18 08:59:45,125] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE forward /tmp/ipykernel_223465/2649045773.py line 27 \n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]  28           0 LOAD_FAST                0 (self)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]               2 LOAD_METHOD              0 (fc1)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]               4 LOAD_FAST                1 (x)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]               6 CALL_METHOD              1\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]               8 STORE_FAST               1 (x)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG] \n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]  29          10 LOAD_GLOBAL              1 (torch)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]              12 LOAD_ATTR                2 (nn)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]              14 LOAD_ATTR                3 (functional)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]              16 LOAD_METHOD              4 (gelu)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]              18 LOAD_FAST                1 (x)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]              20 CALL_METHOD              1\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]              22 STORE_FAST               1 (x)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG] \n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]  30          24 LOAD_FAST                1 (x)\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG]              26 RETURN_VALUE\n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG] \n",
      "[2023-08-18 08:59:45,127] torch._dynamo.convert_frame.__bytecode: [DEBUG] \n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE forward /tmp/ipykernel_223465/2649045773.py line 27 \n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG]  27           0 LOAD_GLOBAL              5 (__compiled_fn_2)\n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG]               2 LOAD_FAST                1 (x)\n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG]               4 CALL_FUNCTION            1\n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG]               6 UNPACK_SEQUENCE          1\n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG]               8 RETURN_VALUE\n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG] \n",
      "[2023-08-18 08:59:45,128] torch._dynamo.convert_frame.__bytecode: [DEBUG] \n",
      "[2023-08-18 08:59:45,130] torch.fx.experimental.symbolic_shapes: [INFO] 2.0: produce_guards\n",
      "[2023-08-18 08:59:45,131] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['x'].size()[0] == 8\n",
      "[2023-08-18 08:59:45,131] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['x'].size()[1] == 32\n",
      "[2023-08-18 08:59:45,132] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['x'].stride()[0] == 32\n",
      "[2023-08-18 08:59:45,133] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['x'].stride()[1] == 1\n",
      "[2023-08-18 08:59:45,133] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['x'].storage_offset() == 0\n",
      "[2023-08-18 08:59:45,136] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      "[2023-08-18 08:59:45,136] torch._dynamo.convert_frame.__guards: [DEBUG]   hasattr(L['x'], '_dynamo_dynamic_indices') == False\n",
      "[2023-08-18 08:59:45,136] torch._dynamo.convert_frame.__guards: [DEBUG]   ___check_obj_id(L['self'], 140688364880656)\n",
      "[2023-08-18 08:59:45,136] torch._dynamo.convert_frame.__guards: [DEBUG]   ___is_grad_enabled()\n",
      "[2023-08-18 08:59:45,136] torch._dynamo.convert_frame.__guards: [DEBUG]   not ___are_deterministic_algorithms_enabled()\n",
      "[2023-08-18 08:59:45,136] torch._dynamo.convert_frame.__guards: [DEBUG]   ___is_torch_function_enabled()\n",
      "[2023-08-18 08:59:45,136] torch._dynamo.convert_frame.__guards: [DEBUG]   utils_device.CURRENT_DEVICE == None\n",
      "STAGE:2023-08-18 08:59:45 223465:223465 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-08-18 08:59:45 223465:223465 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch._dynamo\n",
    "import logging\n",
    "torch._logging.set_logs(dynamo = logging.DEBUG,\n",
    "                        dynamic = logging.DEBUG,\n",
    "                        inductor = logging.DEBUG,\n",
    "                        output_code=True,\n",
    "                        guards=True,\n",
    "                        bytecode=True,\n",
    "                        graph=True,\n",
    "                        graph_code=True,\n",
    "                        aot_graphs=True,\n",
    "                        aot_joint_graph=True,\n",
    "                        aot=logging.DEBUG,\n",
    "                        graph_sizes=True,\n",
    "                        recompiles=True,\n",
    "                        graph_breaks=True,\n",
    "                        schedule=True,\n",
    "                        perf_hints=True)\n",
    "\n",
    "### Simple module:\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "bs = 8\n",
    "input = torch.randn(bs, 32, device=\"cuda\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Trigger compilation on forwards on first run\n",
    "with torch.profiler.profile() as prof: \n",
    "    fn = torch.compile(backend='inductor')(model)\n",
    "    out = fn(input)\n",
    "    out.sum().backward\n",
    "\n",
    "prof.export_chrome_trace(\"MLP_inductor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb72c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
